{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Fri Jul 09 09:39 2021\n",
    "\n",
    "This script is to transform the mean profiles to 2D fields to accelerate the tuning process\n",
    "\n",
    "@author: Clara Burgard\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multimelt.useful_functions as uf\n",
    "import multimelt.melt_functions as meltf\n",
    "from scipy import stats\n",
    "from dask import delayed\n",
    "\n",
    "import distributed\n",
    "import glob\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client(n_workers=8,dashboard_address=':8795', local_directory='/tmp', memory_limit='6GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-bookmark",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87efda-2933-4a07-aff3-8410aba39919",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = 'ACCESS-CM2' # 'EPM026','EPM031', 'EPM034'\n",
    "scenario = 'historical'\n",
    "\n",
    "if mod in ['CNRM-CM6-1','CNRM-ESM2-1']:\n",
    "    to2300 = False\n",
    "elif mod in ['GISS-E2-1-H']:\n",
    "    to2300 = True\n",
    "elif mod in ['ACCESS-CM2','ACCESS-ESM1-5','CESM2-WACCM','CanESM5','IPSL-CM6A-LR','MRI-ESM2-0']:\n",
    "    to2300 = True\n",
    "elif mod in ['MPI-ESM1-2-HR','CESM2','GFDL-CM4','GFDL-ESM4']:\n",
    "    to2300 = False\n",
    "elif mod == 'UKESM1-0-LL':\n",
    "    to2300 = True        \n",
    "\n",
    "if scenario == 'historical':\n",
    "    yystart = 1980 #1850\n",
    "    yyend = 2014\n",
    "else:\n",
    "    if to2300:\n",
    "        yystart = 2015\n",
    "        yyend = 2300\n",
    "    else:\n",
    "        yystart = 2015\n",
    "        yyend = 2100        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data='/bettik/burgardc/DATA/SUMMER_PAPER/interim/'\n",
    "inputpath_profiles='/bettik/burgardc/DATA/SUMMER_PAPER/interim/T_S_PROF/CMIP/'+mod+'/'\n",
    "inputpath_mask='/bettik/burgardc/DATA/SUMMER_PAPER/interim/ANTARCTICA_IS_MASKS/BedMachine_4km/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the domain a little smaller to make the computation even more efficient - file isf has already been made smaller at its creation\n",
    "map_lim = [-3000000,3000000]\n",
    "\n",
    "chunk_size = 600\n",
    "Nisf_chunk = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-cookie",
   "metadata": {},
   "source": [
    "Files for param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b9a7c-f5ee-4eef-b4ca-8beb0af29749",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_isf='/bettik/burgardc/DATA/SUMMER_PAPER/interim/ANTARCTICA_IS_MASKS/BedMachine_4km/'\n",
    "file_isf_orig = xr.open_mfdataset(inputpath_isf+'BedMachinev2_4km_isf_masks_and_info_and_distance_oneFRIS.nc')\n",
    "nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "rignot_isf = file_isf_nonnan.Nisf.where(np.isfinite(file_isf_nonnan['isf_area_rignot']), drop=True)\n",
    "file_isf = file_isf_nonnan.sel(Nisf=rignot_isf).chunk(chunks={'x': chunk_size, 'y': chunk_size})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361d1bd-a122-49a5-9725-512c5cd3173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BedMachine_orig = xr.open_mfdataset(inputpath_data+'BedMachine_v2_aggregated4km_allvars.nc')\n",
    "BedMachine_orig_cut = BedMachine_orig.sel(x=file_isf.x, y=file_isf.y).chunk(chunks={'x': chunk_size, 'y': chunk_size})\n",
    "\n",
    "ice_draft_pos = (BedMachine_orig_cut['thickness'] - BedMachine_orig_cut['surface']).where(file_isf['ISF_mask'] > 1)\n",
    "ice_draft_isf = ice_draft_pos.where(file_isf['ISF_mask'] == file_isf.Nisf)#.chunk(chunks={'Nisf': Nisf_chunk}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec60f33-9251-4516-a80b-a3666899da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tt in tqdm(range(yystart,yyend + 1)):\n",
    "#for tt in tqdm(range(yystart,2063 + 1)):\n",
    "\n",
    "    file_T_orig = xr.open_mfdataset(inputpath_profiles+'T_mean_prof_50km_contshelf_'+mod+'_'+scenario+'_'+str(tt)+'.nc') #, chunks={'Nisf': Nisf_chunk})#, chunks={'Nisf': 1,'x': chunk_size, 'y': chunk_size})\n",
    "    file_S_orig = xr.open_mfdataset(inputpath_profiles+'S_mean_prof_50km_contshelf_'+mod+'_'+scenario+'_'+str(tt)+'.nc') #, chunks={'Nisf': Nisf_chunk})#, chunks={'Nisf': 1,'x': chunk_size, 'y': chunk_size})\n",
    "    file_TS_orig = xr.merge([file_T_orig,file_S_orig])\n",
    "    file_TS_orig = file_TS_orig.assign_coords({'z': -1*file_TS_orig['z'].values}).rename({'z':'depth'})\n",
    "    file_TS = file_TS_orig.sel(Nisf=rignot_isf)#.chunk(chunks={'depth': 20, 'Nisf': 1})\n",
    "    \n",
    "    depth_axis_old = file_TS.depth.values\n",
    "    depth_axis_new = np.concatenate((np.zeros(1),depth_axis_old))\n",
    "\n",
    "    file_TS_with_shallow = file_TS.interp({'depth': depth_axis_new})\n",
    "    filled_TS = file_TS_with_shallow.ffill(dim='depth').bfill(dim='depth') #, 'profile_domain': 1})\n",
    "\n",
    "    # ice draft depth or deepest entrance depth\n",
    "    depth_of_int0 = ice_draft_isf.where(ice_draft_isf<file_isf['front_bot_depth_max'], \n",
    "                                       file_isf['front_bot_depth_max'])\n",
    "    depth_of_int = depth_of_int0.where(ice_draft_isf>file_isf['front_ice_depth_min'], \n",
    "                                       file_isf['front_ice_depth_min']) #.chunk(chunks={'Nisf': Nisf_chunk})\n",
    "    depth_of_int = depth_of_int.where(file_isf['ISF_mask'] == file_isf.Nisf, 0)#.chunk({'Nisf': Nisf_chunk})\n",
    "\n",
    "        \n",
    "    #print('here1')\n",
    "    T_isf = filled_TS['thetao'].interp({'depth': depth_of_int}).drop('depth')\n",
    "    T_isf = T_isf.where(file_isf['ISF_mask']==file_isf.Nisf).sum('Nisf')#.where(depth_of_int>0)\n",
    "    T_isf = T_isf.to_dataset(name='theta_in')\n",
    "    #print('here2')\n",
    "    T_isf['salinity_in'] = filled_TS['so'].interp({'depth': depth_of_int}).drop('depth')\n",
    "    T_isf['salinity_in'] = T_isf['salinity_in'].where(file_isf['ISF_mask']==file_isf.Nisf).sum('Nisf')#.where(depth_of_int>0)\n",
    "    #print('here3')\n",
    "    depth_of_int = depth_of_int.where(file_isf['ISF_mask'] > 1).sum('Nisf')\n",
    "    depth_of_int = depth_of_int.where(depth_of_int>0)\n",
    "    #T_isf['freezing_T'] = meltf.freezing_temperature(T_isf['salinity_in'], -1*depth_of_int)\n",
    "    #print('here4')\n",
    "    #T_isf['thermal_forcing'] = T_isf['theta_in'] - T_isf['freezing_T']\n",
    "    T_isf['depth_of_int'] = depth_of_int\n",
    "    \n",
    "    # project it on 2D\n",
    "    #T_isf = T_isf.sum('Nisf')\n",
    "    T_isf.assign_coords({'time':tt}).to_netcdf(inputpath_profiles+'T_S_2D_fields_isf_draft_'+mod+'_'+scenario+'_'+str(tt)+'.nc','w')\n",
    "    del T_isf\n",
    "    del depth_of_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101fa810-4197-4d86-9d66-1e80dcb1d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e992c2-7d8f-4ae6-aa5e-7b4b4ec01571",
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_list = []\n",
    "for tt in range(yy_start,yy_end+1):\n",
    "    ds_tt = xr.open_dataset(inputpath_profiles+'T_S_2D_fields_isf_draft_'+mod+'_'+scenario+'_'+str(tt)+'.nc')\n",
    "    TS_list.append(ds_tt)\n",
    "TS_ds = xr.concat(TS_list, dim='time')\n",
    "TS_ds.to_netcdf(inputpath_profiles+'T_S_2D_fields_isf_draft_'+mod+'_'+scenario+'_allyy.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
