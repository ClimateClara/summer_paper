{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Apr 13 14:17 2022\n",
    "\n",
    "Prepare csv for individual runs \n",
    "\n",
    "Author: @claraburgard\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "FOR EACH POINT:\n",
    "- T and S extrapolated to ice draft depth\n",
    "- T and S mean\n",
    "- Distance to front\n",
    "- Distance to the grounding line\n",
    "- ice draft zonal and meridional slope in x- and y-direction\n",
    "- bedrock zonal and meridional slope in x- and y-direction\n",
    "- Ice draft depth\n",
    "- Bathymetry\n",
    "- utide\n",
    "- Ice draft concentration\n",
    "- Max bathymetry \n",
    "- Target: melt m ice per yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import summer_paper.data_formatting_NN as dfmt\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = 'historical' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inputpath_data='/bettik/burgardc/DATA/SUMMER_PAPER/interim/'\n",
    "inputpath_mask='/bettik/burgardc/DATA/SUMMER_PAPER/interim/ANTARCTICA_IS_MASKS/BedMachine_4km/'\n",
    "inputpath_boxes = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/BOXES/BedMachine_4km/'\n",
    "inputpath_plumes = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/PLUMES/BedMachine_4km/'\n",
    "\n",
    "\n",
    "\n",
    "outputpath = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_lim = [-3000000,3000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dIF, dGL\n",
    "inputpath_isf='/bettik/burgardc/DATA/SUMMER_PAPER/interim/ANTARCTICA_IS_MASKS/BedMachine_4km/'\n",
    "file_isf_orig = xr.open_dataset(inputpath_isf+'BedMachinev2_4km_isf_masks_and_info_and_distance_oneFRIS.nc')\n",
    "nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "rignot_isf = file_isf_nonnan.Nisf.where(np.isfinite(file_isf_nonnan['isf_area_rignot']), drop=True)\n",
    "file_isf = file_isf_nonnan.sel(Nisf=rignot_isf)\n",
    "\n",
    "# bathymetry, ice draft, concentration\n",
    "BedMachine_orig = xr.open_dataset(inputpath_data+'BedMachine_v2_aggregated4km_allvars.nc')\n",
    "BedMachine_orig_cut = dfmt.cut_domain_stereo(BedMachine_orig, map_lim, map_lim)\n",
    "file_bed_goodGL = -1*BedMachine_orig_cut['bed']\n",
    "file_draft = (BedMachine_orig_cut['thickness'] - BedMachine_orig_cut['surface']).where(file_isf['ISF_mask'] > 1)\n",
    "file_isf_conc = BedMachine_orig_cut['isf_conc']\n",
    "\n",
    "# ice and bed slopes\n",
    "file_slope = xr.open_dataset(inputpath_mask+'BedMachine_4km_slope_info_bedrock_draft_latlon_oneFRIS.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod in ['CNRM-CM6-1']: #'ACCESS-CM2', # continue at 2169'ACCESS-ESM1-5','CESM2','CESM2-WACCM','CNRM-CM6-1','CNRM-ESM2-1',\n",
    "           #'CanESM5','GFDL-CM4','GFDL-ESM4','GISS-E2-1-H','IPSL-CM6A-LR','MPI-ESM1-2-HR',\n",
    "           #'MRI-ESM2-0','UKESM1-0-LL'\n",
    "    \n",
    "    print(mod)\n",
    "    \n",
    "    inputpath_profiles='/bettik/burgardc/DATA/SUMMER_PAPER/interim/T_S_PROF/CMIP/'+mod+'/'\n",
    "    outputpath_nn = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/INPUT_DATA/CMIP/'+mod+'/'\n",
    "\n",
    "    if mod in ['CNRM-CM6-1','CNRM-ESM2-1']:\n",
    "        to2300 = False\n",
    "    elif mod in ['GISS-E2-1-H']:\n",
    "        to2300 = True\n",
    "    elif mod in ['ACCESS-CM2','ACCESS-ESM1-5','CESM2-WACCM','CanESM5','IPSL-CM6A-LR','MRI-ESM2-0']:\n",
    "        to2300 = True\n",
    "    elif mod in ['MPI-ESM1-2-HR','CESM2','GFDL-CM4','GFDL-ESM4']:\n",
    "        to2300 = False\n",
    "    elif mod == 'UKESM1-0-LL':\n",
    "        to2300 = True        \n",
    "\n",
    "    if scenario == 'historical':\n",
    "        yystart = 1980 #1850\n",
    "        yyend = 2014\n",
    "    else:\n",
    "        if to2300:\n",
    "            yystart = 2015\n",
    "            yyend = 2300\n",
    "        else:\n",
    "            yystart = 2015\n",
    "            yyend = 2100   \n",
    "    \n",
    "    if scenario == 'historical':\n",
    "        # T and S extrapolated to ice draft depth\n",
    "        T_S_2D_isfdraft = xr.open_mfdataset(inputpath_profiles+'T_S_2D_fields_isf_draft_'+mod+'_'+scenario+'_*.nc', combine='nested', concat_dim='time') #, chunks=({'time': 5}\n",
    "\n",
    "    # T and S mean and std\n",
    "    T_S_2D_meanstd = xr.open_dataset(inputpath_profiles+'T_S_2D_meanstd_isf_draft_'+mod+'_'+scenario+'.nc')\n",
    "            \n",
    "    for tt in tqdm(range(yystart,yyend+1)):\n",
    "    #for tt in tqdm(range(2032,yyend+1)):\n",
    "        \n",
    "        if scenario == 'historical':\n",
    "            T_S_2D_isfdraft_tt = T_S_2D_isfdraft.sel(time=tt).load()\n",
    "        else:\n",
    "            T_S_2D_isfdraft_tt = xr.open_dataset(inputpath_profiles+'T_S_2D_fields_isf_draft_'+mod+'_'+scenario+'_'+str(tt)+'.nc')\n",
    "        T_S_2D_meanstd_tt = T_S_2D_meanstd.sel(time=tt)\n",
    "\n",
    "        time_dpdt_in = file_isf[['dGL', 'dIF']].merge(file_draft.rename('corrected_isfdraft')\n",
    "                                                     ).merge(file_bed_goodGL.rename('bathy_metry')\n",
    "                                                            ).merge(file_slope).merge(file_isf_conc).merge(T_S_2D_isfdraft_tt[['theta_in','salinity_in']]).merge(T_S_2D_meanstd_tt)\n",
    "\n",
    "        time_dpdt_in['dIF'] = time_dpdt_in['dIF'].where(np.isfinite(time_dpdt_in['dIF']), np.nan)\n",
    "\n",
    "        li = []\n",
    "        for kisf in file_isf.Nisf:\n",
    "            ds_kisf = time_dpdt_in.where(file_isf['ISF_mask'] == kisf, drop=True)\n",
    "\n",
    "            df_kisf = ds_kisf.drop('longitude').drop('latitude').to_dataframe()\n",
    "            li.append(df_kisf)\n",
    "\n",
    "        df_allkisf = pd.concat(li)\n",
    "        # remove rows where there are nans\n",
    "        clean_df_allkisf = df_allkisf.dropna()\n",
    "        clean_df_allkisf = clean_df_allkisf .where(clean_df_allkisf ['salinity_in']!=0).dropna()\n",
    "        clean_df_tt = clean_df_allkisf.set_index(['time'], append=True)\n",
    "        clean_df_tt.to_csv(outputpath_nn + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt)+'.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT NEEDED AFTER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for kisf in file_isf.Nisf:\n",
    "        ds_kisf = time_dpdt_in.where(file_isf['ISF_mask'] == kisf, drop=True)\n",
    "\n",
    "        df_kisf = ds_kisf.drop('longitude').drop('latitude').to_dataframe()\n",
    "        # remove rows where there are nans\n",
    "        clean_df_kisf = df_kisf.dropna()\n",
    "        clean_df_kisf = clean_df_kisf.where(clean_df_kisf['salinity_in']!=0).dropna()\n",
    "        clean_df_kisf['time'] = tt\n",
    "        #clean_df_kisf.to_csv(outputpath_nn + 'dataframe_input_isf'+str(kisf.values).zfill(3)+'_'+mod+'_'+scenario+'_'+str(tt)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kisf in file_isf.Nisf:\n",
    "    ds_kisf = time_dpdt_in.where(file_isf['ISF_mask'] == kisf, drop=True)\n",
    "\n",
    "    df_kisf = ds_kisf.drop('longitude').drop('latitude').to_dataframe()\n",
    "    # remove rows where there are nans\n",
    "    clean_df_kisf = df_kisf.dropna()\n",
    "    clean_df_kisf = clean_df_kisf.where(clean_df_kisf['salinity_in']!=0).dropna()\n",
    "    clean_df_kisf['time'] = clean_df_kisf['time'].dt.year\n",
    "    clean_df_kisf.to_csv(outputpath_nn + 'dataframe_input_isf'+str(kisf.values).zfill(3)+'_'+mod+'_'+scenario+'_'+str(tt)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dpdt_in.where(file_isf['ISF_mask'] == kisf, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "PREPARE ONE DATASET WITH EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dpdt_in.merge(u_tide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_2D_br, time_dpdt_in_br = xr.broadcast(geometry_2D,time_dpdt_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_input_xr = xr.merge([geometry_2D_br, time_dpdt_in_br]).transpose('y','x','time').drop('profile_domain').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "PREPARE CLEAN DATAFRAME WITH ALL DATA TO SAVE AND FEED TO THE NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kisf in tqdm(file_isf.Nisf):\n",
    "    ds_kisf = final_input_xr.where(file_isf['ISF_mask'] == kisf, drop=True).drop('Nisf')\n",
    "    \n",
    "    df_kisf = ds_kisf.drop('longitude').drop('latitude').to_dataframe()\n",
    "    # remove rows where there are nans\n",
    "    clean_df_kisf = df_kisf.dropna()\n",
    "    clean_df_kisf = clean_df_kisf.where(clean_df_kisf['salinity_in']!=0).dropna()\n",
    "    clean_df_kisf.to_csv(outputpath_nn + 'dataframe_input_isf'+str(kisf.values).zfill(3)+'_'+nemo_run0+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_kisf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "SAVE TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_yy.to_csv(outputpath_nn + 'dataframe_input_'+nemo_run+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove index (time, x, y)\n",
    "clean_df_yy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "PREPARE LAT AND LON FOR A CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon = file_isf[['latitude', 'longitude']].reset_coords(names=['longitude','latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_br, salinity_for_nans = xr.broadcast(latlon,T_S_2D_isfdraft['salinity_in'].drop(['longitude','latitude']))\n",
    "latlon_input_xr = xr.merge([latlon_br, salinity_for_nans]).transpose('y','x','time').drop('profile_domain').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kisf in tqdm(file_isf.Nisf):\n",
    "    ds_kisf = latlon_input_xr.where(file_isf['ISF_mask'] == kisf, drop=True).drop('Nisf')\n",
    "    df_kisf = ds_kisf.to_dataframe()\n",
    "    # remove rows where there are nans\n",
    "    clean_df_kisf = df_kisf.dropna()\n",
    "    clean_df_kisf = clean_df_kisf.where(clean_df_kisf['salinity_in']!=0).dropna()\n",
    "    #clean_df_kisf.to_csv(outputpath_nn + 'dataframe_input_isf'+str(kisf.values).zfill(3)+'_'+nemo_run0+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
