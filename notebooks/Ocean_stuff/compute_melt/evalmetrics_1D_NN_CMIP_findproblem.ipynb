{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5dfe2-7fa8-458b-a30c-e0a11be8d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Mar 27 17:37 2023\n",
    "\n",
    "Apply the ensemble of NN to Smith data and only take ensemble mean\n",
    "\n",
    "Author: @claraburgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731f235-1948-4d8e-8bca-d6206005bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "#from tqdm import tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "import os,sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from multimelt.constants import *\n",
    "import multimelt.useful_functions as uf\n",
    "import summer_paper.data_formatting_NN as dfmt\n",
    "import summer_paper.postprocessing_functions_NN as pp\n",
    "\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9625ca-6f3d-4c7b-ad41-625d54472bcf",
   "metadata": {},
   "source": [
    "DEFINE OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ce0dc-b22d-4f8f-b178-3b74e9dd35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod_size =  'xsmall96' #'mini', 'small', 'medium', 'large', 'extra_large'\n",
    "TS_opt = 'extrap' # extrap, whole, thermocline\n",
    "norm_method =  'std' # std,interquart, minmax\n",
    "exp_name = 'newbasic2'#'allbutconstants' #'onlyTSdraftandslope' #'TSdraftbotandiceddandwcd' #'onlyTSisfdraft' #'TSdraftbotandiceddandwcdreldGL' #TSdraftslopereldGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f22c01-07e7-47de-90e8-c5d9643eb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath_info = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/'\n",
    "\n",
    "nemo_run = 'ctrl94' # 'EPM031', 'EPM034'\n",
    "\n",
    "file_info = pd.read_csv(outputpath_info+'info_chunks.txt', delimiter=',', header=None)\n",
    "file_info = file_info.set_index(file_info[0])\n",
    "\n",
    "for chunk_nb in file_info[file_info[1]==nemo_run][0].values:\n",
    "    start_yy = file_info[file_info[1]==nemo_run][2].loc[chunk_nb]\n",
    "    end_yy = file_info[file_info[1]==nemo_run][3].loc[chunk_nb]\n",
    "    trange = range(start_yy,end_yy+1)\n",
    "    print(chunk_nb,start_yy,end_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f23b01-1821-488e-a15a-ffab868afb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tblock_dim = [30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54666cc3-7f68-4d51-bc27-1bf2acc53e52",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99fc552-7135-4372-bd4c-409ce8f63e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '/bettik/burgardc/'\n",
    "\n",
    "inputpath_data_nn = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/'\n",
    "inputpath_csv = inputpath_data_nn+'EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "inputpath_mask = home_path+'/DATA/SUMMER_PAPER/interim/ANTARCTICA_IS_MASKS/nemo_5km_'+nemo_run+'/'\n",
    "inputpath_data=home_path+'/DATA/SUMMER_PAPER/interim/'\n",
    "inputpath_CVinput = home_path+'/DATA/NN_PARAM/interim/INPUT_DATA/EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "inputpath_plumes = home_path+'/DATA/SUMMER_PAPER/interim/PLUMES/nemo_5km_'+nemo_run+'/'\n",
    "inputpath_boxes = home_path+'/DATA/SUMMER_PAPER/interim/BOXES/nemo_5km_'+nemo_run+'/'\n",
    "\n",
    "# make the domain a little smaller to make the computation even more efficient - file isf has already been made smaller at its creation\n",
    "map_lim = [-3000000,3000000]\n",
    "\n",
    "file_isf, geometry_info_2D, box_charac_2D, box_charac_1D, isf_stack_mask = pp.read_input_evalmetrics_NN(nemo_run)\n",
    "\n",
    "\n",
    "norm_metrics_file = xr.open_dataset(inputpath_CVinput + 'metrics_norm_wholedataset_origexcept26_christoph_new.nc')\n",
    "norm_metrics = norm_metrics_file.to_dataframe()\n",
    "\n",
    "box_loc_config2 = box_charac_2D['box_location'].sel(box_nb_tot=box_charac_1D['nD_config'].sel(config=2))\n",
    "box1 = box_loc_config2.where(box_loc_config2==1).isel(Nisf=1).drop('Nisf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a5933-b426-4e36-83af-ee3298233683",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_size = 'xsmall96'\n",
    "\n",
    "### APPLY MODEL\n",
    "\n",
    "input_vars = ['dGL','dIF','corrected_isfdraft','bathy_metry','slope_bed_lon','slope_bed_lat','slope_ice_lon','slope_ice_lat',\n",
    "                'theta_in','salinity_in','T_mean', 'S_mean', 'T_std', 'S_std']\n",
    "\n",
    "tuning_sort = 'new' #'old'\n",
    "\n",
    "### use any model from CV over time\n",
    "#outputpath_melt = home_path+'/DATA/SUMMER_PAPER/processed/OCEAN_MELT_RATE_CMIP/'+mod+'/'\n",
    "if tuning_sort == 'new':\n",
    "    path_model = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/NN_MODELS/'\n",
    "elif tuning_sort == 'old':\n",
    "    path_model = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/experiments/WHOLE/'\n",
    "\n",
    "def apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_nrun, model, input_vars=[]):\n",
    "    \"\"\"\n",
    "    Compute 2D melt based on a given NN model\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    val_norm = pp.normalise_vars(df_nrun[input_vars],\n",
    "                                norm_metrics[input_vars].loc['mean_vars'],\n",
    "                                norm_metrics[input_vars].loc['range_vars'])\n",
    "\n",
    "    x_val_norm = val_norm\n",
    "\n",
    "    y_out_norm = model.predict(x_val_norm.values.astype('float64'),verbose = 0)\n",
    "\n",
    "    y_out_norm_xr = xr.DataArray(data=y_out_norm.squeeze()).rename({'dim_0': 'index'})\n",
    "    y_out_norm_xr = y_out_norm_xr.assign_coords({'index': x_val_norm.index})\n",
    "\n",
    "    # denormalise the output\n",
    "    y_out = pp.denormalise_vars(y_out_norm_xr, \n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['mean_vars'],\n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['range_vars'])\n",
    "\n",
    "    y_out_pd_s = pd.Series(y_out.values,index=df_nrun.index,name='predicted_melt') \n",
    "\n",
    "    # put some order in the file\n",
    "    y_out_xr = y_out_pd_s.to_xarray().sortby('y')\n",
    "\n",
    "    y_whole_grid = y_out_xr.reindex_like(file_isf['ISF_mask'])\n",
    "    return y_whole_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f5d80-7def-4d47-a827-91fdc7e92743",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('NN size',mod_size)\n",
    "    seed_nb = 1\n",
    "    model = keras.models.load_model(path_model + 'model_nn_'+mod_size+'_'+exp_name+'_wholedataset_'+str(seed_nb).zfill(2)+'_TS'+TS_opt+'_norm'+norm_method+'.h5')\n",
    "    \n",
    "    tblock=30\n",
    "    li = []\n",
    "    for kisf in tqdm(file_isf.Nisf): \n",
    "        df_nrun = pd.read_csv(inputpath_csv + 'dataframe_input_isf'+str(kisf.values).zfill(3)+'_'+str(tblock).zfill(3)+'_new.csv',index_col=[0,1,2])\n",
    "        li.append(df_nrun)\n",
    "    df_allyy = pd.concat(li)\n",
    "    \n",
    "    print('Computing 2D patterns')\n",
    "    xr_ensmean_res2D = apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_allyy, model, input_vars)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73164b-5504-4502-8174-20cbe09a40bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyy.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a0bbd-cb1a-4db0-b255-4baa1f34ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyy.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628bdf8-766a-4e54-ae75-f553e6fefa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LarsenB = df_nrun.where(df_allyy['Nisf'] == 66).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304361aa-84c2-4114-b700-d3fb59d574bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ensmean_res2D = apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_LarsenB, model, input_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7e273-6935-40ab-983b-c2294aae3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ensmean_res2D.where(np.isfinite(xr_ensmean_res2D), drop=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e001f-df0e-43cb-bc54-1bd0af7c87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = df_LarsenB['theta_in'].to_xarray().sortby('y').reindex_like(file_isf['ISF_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92a1ca-8edb-4bfc-bde7-9252f932dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var.where(file_isf['ISF_mask'] == 66, drop=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36266c8-e4bf-4483-b7fc-4884fb0283df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_profiles='/bettik/burgardc/DATA/SUMMER_PAPER/interim/T_S_PROF/CMIP/'\n",
    "\n",
    "S_prof = xr.open_dataset(inputpath_profiles + mod + '/S_mean_prof_50km_contshelf_'+mod+'_'+scenario+'_1980.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79517f2f-d1ee-48bb-9ef4-0c11ef5517f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_prof['so'].sel(Nisf=66).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb356959-af22-42f7-975c-d791da058dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data_orig='/bettik/jourdai1/OCEAN_DATA_CMIP6_STEREO/'+mod+'/'\n",
    "T_ocean_files = xr.open_mfdataset(inputpath_data_orig+'so_Oyr_'+mod+'_'+scenario+'_r1i1p1f2_1950*.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889f04e-ede8-43d0-a8f0-ca5a717829f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_ocean_files['so'].sel(time='1980-07-01T14:00:00.000000000').isel(z=6).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9cb10-08ac-4fde-9096-93a28f1a4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPLY MODEL\n",
    "\n",
    "input_vars = ['dGL','dIF','corrected_isfdraft','bathy_metry','slope_bed_lon','slope_bed_lat','slope_ice_lon','slope_ice_lat',\n",
    "                'theta_in','salinity_in','T_mean', 'S_mean', 'T_std', 'S_std']\n",
    "\n",
    "tuning_sort = 'new' #'old'\n",
    "\n",
    "### use any model from CV over time\n",
    "outputpath_melt = home_path+'/DATA/SUMMER_PAPER/processed/OCEAN_MELT_RATE_CMIP/'+mod+'/'\n",
    "if tuning_sort == 'new':\n",
    "    path_model = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/NN_MODELS/'\n",
    "elif tuning_sort == 'old':\n",
    "    path_model = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/experiments/WHOLE/'\n",
    "\n",
    "def apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_nrun, model, input_vars=[]):\n",
    "    \"\"\"\n",
    "    Compute 2D melt based on a given NN model\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    val_norm = pp.normalise_vars(df_nrun[input_vars],\n",
    "                                norm_metrics[input_vars].loc['mean_vars'],\n",
    "                                norm_metrics[input_vars].loc['range_vars'])\n",
    "\n",
    "    x_val_norm = val_norm\n",
    "\n",
    "    y_out_norm = model.predict(x_val_norm.values.astype('float64'),verbose = 0)\n",
    "\n",
    "    y_out_norm_xr = xr.DataArray(data=y_out_norm.squeeze()).rename({'dim_0': 'index'})\n",
    "    y_out_norm_xr = y_out_norm_xr.assign_coords({'index': x_val_norm.index})\n",
    "\n",
    "    # denormalise the output\n",
    "    y_out = pp.denormalise_vars(y_out_norm_xr, \n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['mean_vars'],\n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['range_vars'])\n",
    "\n",
    "    y_out_pd_s = pd.Series(y_out.values,index=df_nrun.index,name='predicted_melt') \n",
    "\n",
    "    # put some order in the file\n",
    "    y_out_xr = y_out_pd_s.to_xarray().sortby('y')\n",
    "\n",
    "    y_whole_grid = y_out_xr.reindex_like(file_isf['ISF_mask'])\n",
    "    return y_whole_grid\n",
    "\n",
    "### If no ensemble of models\n",
    "\n",
    "for mod_size in ['xsmall96','small','large']: #\n",
    "    \n",
    "    print('NN size',mod_size)\n",
    "    seed_nb = 1\n",
    "    model = keras.models.load_model(path_model + 'model_nn_'+mod_size+'_'+exp_name+'_wholedataset_'+str(seed_nb).zfill(2)+'_TS'+TS_opt+'_norm'+norm_method+'.h5')\n",
    "    \n",
    "    if scenario == 'historical':\n",
    "        print('prepare 2D input')\n",
    "        li = []\n",
    "        for tt in tqdm(range(yystart,yyend+1)):    \n",
    "            #print('Combining all dataframes')\n",
    "        \n",
    "            df_nrun = pd.read_csv(inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt)+'.csv',index_col=[0,1,2])\n",
    "            li.append(df_nrun)\n",
    "\n",
    "        print('Concatenating input')\n",
    "        df_allyy = pd.concat(li)\n",
    "        \n",
    "        print('Computing 2D patterns')\n",
    "        xr_ensmean_res2D = apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_allyy, model, input_vars)\n",
    "\n",
    "        print('Compute the 1D evalmetrics')\n",
    "        res_1D_list = []\n",
    "        for kisf in tqdm(file_isf.Nisf.values): \n",
    "\n",
    "            geometry_isf_2D = dfmt.choose_isf(geometry_info_2D,isf_stack_mask, kisf)\n",
    "            melt_rate_2D_isf_m_per_y = dfmt.choose_isf(xr_ensmean_res2D,isf_stack_mask, kisf)\n",
    "\n",
    "            melt_rate_1D_isf_Gt_per_y = (melt_rate_2D_isf_m_per_y * geometry_isf_2D['grid_cell_area_weighted']).sum(dim=['mask_coord']) * rho_i / 10**12\n",
    "\n",
    "            box_loc_config_stacked = dfmt.choose_isf(box1, isf_stack_mask, kisf)\n",
    "            param_melt_2D_box1_isf = melt_rate_2D_isf_m_per_y.where(np.isfinite(box_loc_config_stacked))\n",
    "\n",
    "            melt_rate_1D_isf_myr_box1_mean = dfmt.weighted_mean(param_melt_2D_box1_isf,['mask_coord'], geometry_isf_2D['grid_cell_area_weighted'])     \n",
    "\n",
    "            out_1D = xr.concat([melt_rate_1D_isf_Gt_per_y, melt_rate_1D_isf_myr_box1_mean], dim='metrics').assign_coords({'metrics': ['Gt','box1']})\n",
    "            res_1D_list.append(out_1D) \n",
    "\n",
    "        res_1D_all = xr.concat(res_1D_list, dim='Nisf')\n",
    "        res_1D_all.rename('predicted_melt').to_netcdf(outputpath_melt + 'evalmetrics_1D_NN'+mod_size+'_'+scenario+'.nc')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        res_1Dtt_list = []\n",
    "        \n",
    "        for tt in tqdm(range(yystart,yyend+1,50)):    \n",
    "            #print('Combining all dataframes')\n",
    "            trange = range(tt,tt+50)\n",
    "            \n",
    "            li = []\n",
    "            for tt0 in trange: \n",
    "                \n",
    "                if os.path.exists(inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt0)+'.csv'):\n",
    "                    df_nrun = pd.read_csv(inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt0)+'.csv',index_col=[0,1,2])\n",
    "                    li.append(df_nrun)  \n",
    "                    ttend = tt0\n",
    "                else:\n",
    "                    print(str(tt0)+' file does not exist')\n",
    "                \n",
    "            trange = range(tt,ttend+1)\n",
    "            df_allyy = pd.concat(li)\n",
    "\n",
    "            #print('Computing 2D patterns')\n",
    "            xr_ensmean_res2D = apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_allyy, model, input_vars)\n",
    "\n",
    "            #print('Compute the 1D evalmetrics')\n",
    "            res_1D_list = []\n",
    "            for kisf in file_isf.Nisf.values: \n",
    "\n",
    "                geometry_isf_2D = dfmt.choose_isf(geometry_info_2D,isf_stack_mask, kisf)\n",
    "                melt_rate_2D_isf_m_per_y = dfmt.choose_isf(xr_ensmean_res2D,isf_stack_mask, kisf)\n",
    "\n",
    "                melt_rate_1D_isf_Gt_per_y = (melt_rate_2D_isf_m_per_y * geometry_isf_2D['grid_cell_area_weighted']).sum(dim=['mask_coord']) * rho_i / 10**12\n",
    "\n",
    "                box_loc_config_stacked = dfmt.choose_isf(box1, isf_stack_mask, kisf)\n",
    "                param_melt_2D_box1_isf = melt_rate_2D_isf_m_per_y.where(np.isfinite(box_loc_config_stacked))\n",
    "\n",
    "                melt_rate_1D_isf_myr_box1_mean = dfmt.weighted_mean(param_melt_2D_box1_isf,['mask_coord'], geometry_isf_2D['grid_cell_area_weighted'])     \n",
    "\n",
    "                out_1D = xr.concat([melt_rate_1D_isf_Gt_per_y, melt_rate_1D_isf_myr_box1_mean], dim='metrics').assign_coords({'metrics': ['Gt','box1']})\n",
    "                res_1D_list.append(out_1D) \n",
    "\n",
    "            res_1D_tt = xr.concat(res_1D_list, dim='Nisf').assign_coords({'time': trange})\n",
    "            res_1Dtt_list.append(res_1D_tt)\n",
    "            \n",
    "        res_1D_all = xr.concat(res_1Dtt_list, dim='time')\n",
    "        res_1D_all.rename('predicted_melt').to_netcdf(outputpath_melt + 'evalmetrics_1D_NN'+mod_size+'_'+scenario+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84e267-8cc4-4ac8-a580-6bb0918b24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt)+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bd21e-28fb-4547-938a-fdce10ed1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt0)+'.csv'):\n",
    "    print('True')\n",
    "else:\n",
    "    print('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be6c7c-743b-4c1b-b685-db2ca7a86266",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt0)+'.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32800147-fc90-4e88-9418-128099d86d57",
   "metadata": {},
   "source": [
    "###############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
