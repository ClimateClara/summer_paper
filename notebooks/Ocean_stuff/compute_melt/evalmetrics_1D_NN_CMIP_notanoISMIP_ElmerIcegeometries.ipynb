{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Mar 27 17:37 2023\n",
    "\n",
    "Apply the ensemble of NN to Smith data and only take ensemble mean\n",
    "\n",
    "Author: @claraburgard\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "import os,sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from multimelt.constants import *\n",
    "import summer_paper.useful_functions as uf\n",
    "import summer_paper.data_formatting_NN as dfmt\n",
    "import summer_paper.postprocessing_functions_NN as pp\n",
    "\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "DEFINE OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod_size =  'xsmall96' #'mini', 'small', 'medium', 'large', 'extra_large'\n",
    "TS_opt = 'extrap' # extrap, whole, thermocline\n",
    "norm_method =  'std' # std,interquart, minmax\n",
    "exp_name = 'newbasic2'#'allbutconstants' #'onlyTSdraftandslope' #'TSdraftbotandiceddandwcd' #'onlyTSisfdraft' #'TSdraftbotandiceddandwcdreldGL' #TSdraftslopereldGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '/bettik/burgardc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = 'GFDL-CM4'\n",
    "scenario = 'historical'\n",
    "\n",
    "if mod in ['CNRM-CM6-1','CNRM-ESM2-1']:\n",
    "    to2300 = False\n",
    "elif mod in ['GISS-E2-1-H']:\n",
    "    to2300 = True\n",
    "elif mod in ['ACCESS-CM2','ACCESS-ESM1-5','CESM2-WACCM','CanESM5','IPSL-CM6A-LR','MRI-ESM2-0']:\n",
    "    to2300 = True\n",
    "elif mod in ['MPI-ESM1-2-HR','GFDL-CM4','GFDL-ESM4']:\n",
    "    to2300 = False\n",
    "elif mod == 'UKESM1-0-LL':\n",
    "    to2300 = True     \n",
    "elif mod == 'CESM2':\n",
    "    to2300 = False \n",
    "\n",
    "if scenario == 'historical':\n",
    "    yystart = 1980\n",
    "    yyend = 2014\n",
    "    #yyend = 1981\n",
    "elif scenario == 'ssp245':\n",
    "    yystart = 2015\n",
    "    yyend = 2100  \n",
    "else:\n",
    "    if to2300:\n",
    "        yystart = 2015\n",
    "        yyend = 2300\n",
    "    else:\n",
    "        yystart = 2015\n",
    "        yyend = 2100   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoyear = 2150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '/bettik/burgardc/'\n",
    "\n",
    "inputpath_mask = home_path+'/DATA/SUMMER_PAPER/interim/ANTARCTICA_IS_MASKS/ElmerIce_'+str(geoyear)+'/'\n",
    "inputpath_data=home_path+'/DATA/SUMMER_PAPER/interim/'\n",
    "inputpath_CVinput = home_path+'/DATA/NN_PARAM/interim/INPUT_DATA/EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "inputpath_plumes = home_path+'/DATA/SUMMER_PAPER/interim/PLUMES/ElmerIce_'+str(geoyear)+'/'\n",
    "inputpath_boxes = home_path+'/DATA/SUMMER_PAPER/interim/BOXES/ElmerIce_'+str(geoyear)+'/'\n",
    "\n",
    "# make the domain a little smaller to make the computation even more efficient - file isf has already been made smaller at its creation\n",
    "map_lim = [-3000000,3000000]\n",
    "\n",
    "file_isf_orig = xr.open_dataset(inputpath_mask+'ElmerIce_4km_'+str(geoyear)+'isf_masks_and_info_and_distance_oneFRIS.nc')\n",
    "nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "sorted_isf_rignot = [11,69,43,28,12,57,\n",
    "                     70,44,29,13,58,71,45,30,14,\n",
    "                     59,72,46,\n",
    "                     31,\n",
    "                     15,61,73,47,32,16,48,33,17,62,49,34,18,63,74,\n",
    "                     50,35,19,64,\n",
    "                     10,\n",
    "                     36,20,65,51,37,\n",
    "                     22,38,52,23,66,53,39,24,\n",
    "                     67,40,54,75,25,41,\n",
    "                     26,42,55,68,60,27]\n",
    "file_isf = file_isf_nonnan.sel(Nisf=sorted_isf_rignot)\n",
    "file_isf['isf_name'] = file_isf['isf_name'].astype(str)\n",
    "\n",
    "rignot_isf = sorted_isf_rignot\n",
    "\n",
    "inputpath_ElmerIce='/bettik/burgardc/DATA/SUMMER_PAPER/interim/ELMERICE_NEWGEO/'\n",
    "BedMachine_orig = xr.open_dataset(inputpath_ElmerIce+'ElmerIce_4km_allvars_'+str(geoyear)+'.nc')\n",
    "file_BedMachine = dfmt.cut_domain_stereo(BedMachine_orig, map_lim, map_lim)\n",
    "file_bed_goodGL = -1*file_BedMachine['bed']\n",
    "file_draft = (file_BedMachine['thickness'] - file_BedMachine['surface']).where(file_isf['ISF_mask'] > 1)\n",
    "file_isf_conc = file_BedMachine['isf_conc']\n",
    "\n",
    "grid_cell_area_file = xr.open_dataset(inputpath_data+'gridarea_ISMIP6_AIS_4000m_grid.nc').sel(x=file_isf.x,y=file_isf.y)\n",
    "true_grid_cell_area = grid_cell_area_file['cell_area'].drop_vars('lon').drop_vars('lat')\n",
    "cell_area_weight = true_grid_cell_area/(4000 * 4000)\n",
    "\n",
    "lon = file_isf.longitude\n",
    "lat = file_isf.latitude\n",
    "\n",
    "xx = file_isf.x\n",
    "yy = file_isf.y\n",
    "dx = (xx[2] - xx[1]).values\n",
    "dy = (yy[2] - yy[1]).values\n",
    "grid_cell_area_const = abs(dx*dy)  \n",
    "grid_cell_area_weighted = file_isf_conc * grid_cell_area_const * cell_area_weight\n",
    "\n",
    "ice_draft_pos = file_draft\n",
    "ice_draft_neg = -ice_draft_pos\n",
    "\n",
    "isf_stack_mask = uf.create_stacked_mask(file_isf['ISF_mask'], file_isf.Nisf, ['y','x'], 'mask_coord')\n",
    "\n",
    "box_charac_all_2D = xr.open_dataset(inputpath_boxes + 'ElmerIce_4km_'+str(geoyear)+'_boxes_2D_oneFRIS.nc')\n",
    "box_charac_all_1D = xr.open_dataset(inputpath_boxes + 'ElmerIce_4km_'+str(geoyear)+'_boxes_1D_oneFRIS.nc')\n",
    "plume_charac_old = xr.open_dataset(inputpath_plumes+'ElmerIce_'+str(geoyear)+'_plume_characteristics.nc')\n",
    "plume_charac_new = xr.open_dataset(inputpath_plumes+'ElmerIce_'+str(geoyear)+'_plume_characteristics_lazero_comparison_mixedshift.nc')\n",
    "plume_charac = xr.concat([plume_charac_old.drop_sel(option='lazero'),plume_charac_new.sel(option='new_lazero')], dim='option').assign_coords({'option': ['cavity','local','lazero']})\n",
    "\n",
    "param_var_of_int_2D = file_isf[['ISF_mask', 'latitude', 'longitude', 'dGL']]\n",
    "param_var_of_int_1D = file_isf[['front_bot_depth_avg', 'front_bot_depth_max','isf_name']]\n",
    "\n",
    "geometry_info_2D = plume_charac.merge(param_var_of_int_2D).merge(ice_draft_pos.rename('ice_draft_pos')).merge(grid_cell_area_weighted.rename('grid_cell_area_weighted')).merge(file_isf_conc.rename('isfdraft_conc'))\n",
    "geometry_info_1D = param_var_of_int_1D\n",
    "\n",
    "norm_metrics_file = xr.open_dataset(inputpath_CVinput + 'metrics_norm_wholedataset_origexcept26_christoph_new.nc')\n",
    "norm_metrics = norm_metrics_file.to_dataframe()\n",
    "\n",
    "box_loc_config2 = box_charac_all_2D['box_location'].sel(box_nb_tot=box_charac_all_1D['nD_config'].sel(config=2))\n",
    "box1 = box_loc_config2.where(box_loc_config2==1).isel(Nisf=1).drop('Nisf')\n",
    "\n",
    "file_slope = xr.open_dataset(inputpath_mask+'ElmerIce_4km_'+str(geoyear)+'_slope_info_bedrock_draft_latlon_oneFRIS.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "ONLY ONE ENSEMBLE MEMBER OF THE NN DEEP ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPLY MODEL\n",
    "\n",
    "input_vars = ['dGL','dIF','corrected_isfdraft','bathy_metry','slope_bed_lon','slope_bed_lat','slope_ice_lon','slope_ice_lat',\n",
    "                'theta_in','salinity_in','T_mean', 'S_mean', 'T_std', 'S_std']\n",
    "\n",
    "tuning_sort = 'new' #'old'\n",
    "\n",
    "### use any model from CV over time\n",
    "if tuning_sort == 'new':\n",
    "    path_model = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/NN_MODELS/'\n",
    "elif tuning_sort == 'old':\n",
    "    path_model = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/experiments/WHOLE/'\n",
    "    \n",
    "scenario = 'ssp245'\n",
    "\n",
    "### APPLY MODEL\n",
    "\n",
    "def apply_NN_results_2D_allisf_allyears(file_isf, norm_metrics, all_info_all, model, input_vars=[]):\n",
    "    \n",
    "    all_info_df = all_info_all.to_dataframe()\n",
    "    val_norm = pp.normalise_vars(all_info_df[input_vars],\n",
    "                                norm_metrics[input_vars].loc['mean_vars'],\n",
    "                                norm_metrics[input_vars].loc['range_vars'])\n",
    "\n",
    "    x_val_norm = val_norm\n",
    "\n",
    "    y_out_norm = model.predict(x_val_norm.values.astype('float64'),verbose = 1)\n",
    "\n",
    "    y_out_norm_xr = xr.DataArray(data=y_out_norm.squeeze()).rename({'dim_0': 'index'})\n",
    "    y_out_norm_xr = y_out_norm_xr.assign_coords({'index': x_val_norm.index})\n",
    "\n",
    "    y_out_norm_xr_2D = uf.bring_back_to_2D(y_out_norm_xr)\n",
    "\n",
    "    # denormalise the output\n",
    "    y_out_xr = pp.denormalise_vars(y_out_norm_xr_2D, \n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['mean_vars'],\n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['range_vars'])\n",
    "\n",
    "    y_whole_grid = y_out_xr.reindex_like(file_isf['ISF_mask'])\n",
    "    return y_whole_grid\n",
    "\n",
    "### If no ensemble of models\n",
    "for mod in ['ACCESS-CM2','ACCESS-ESM1-5','CESM2','CESM2-WACCM','CNRM-CM6-1','CNRM-ESM2-1',\n",
    "           'CanESM5','GFDL-CM4','GFDL-ESM4','GISS-E2-1-H','IPSL-CM6A-LR','MPI-ESM1-2-HR',\n",
    "           'MRI-ESM2-0','UKESM1-0-LL']: #,,\n",
    "    print(mod)\n",
    "    \n",
    "    if mod in ['CNRM-CM6-1','CNRM-ESM2-1']:\n",
    "        to2300 = False\n",
    "    elif mod in ['GISS-E2-1-H']:\n",
    "        to2300 = True\n",
    "    elif mod in ['ACCESS-CM2','ACCESS-ESM1-5','CESM2-WACCM','CanESM5','IPSL-CM6A-LR','MRI-ESM2-0']:\n",
    "        to2300 = True\n",
    "    elif mod in ['MPI-ESM1-2-HR','GFDL-CM4','GFDL-ESM4']:\n",
    "        to2300 = False\n",
    "    elif mod == 'UKESM1-0-LL':\n",
    "        to2300 = True     \n",
    "    elif mod == 'CESM2':\n",
    "        to2300 = False        \n",
    "\n",
    "    if scenario == 'historical':\n",
    "        yystart = 1980 #1850\n",
    "        yyend = 2014\n",
    "    elif scenario == 'ssp245':\n",
    "        yystart = 2015\n",
    "        yyend = 2100  \n",
    "    else:\n",
    "        if to2300:\n",
    "            yystart = 2015\n",
    "            yyend = 2300\n",
    "        else:\n",
    "            yystart = 2015\n",
    "            yyend = 2100 \n",
    "\n",
    "    inputpath_profiles='/bettik/burgardc/DATA/SUMMER_PAPER/interim/T_S_PROF/CMIP/'+mod+'/'\n",
    "    outputpath_melt = home_path+'/DATA/SUMMER_PAPER/processed/OCEAN_MELT_RATE_CMIP/'+mod+'/'\n",
    "\n",
    "\n",
    "            \n",
    "    # PREPARE VARIABLES\n",
    "    file_TS_list = []\n",
    "    for tt in range(yystart,yyend+1):\n",
    "        file_T_orig = xr.open_dataset(inputpath_profiles+'T_mean_prof_50km_contshelf_'+mod+'_'+scenario+'_'+str(tt)+'.nc')\n",
    "        file_S_orig = xr.open_dataset(inputpath_profiles+'S_mean_prof_50km_contshelf_'+mod+'_'+scenario+'_'+str(tt)+'.nc')\n",
    "        file_TS_orig = xr.merge([file_T_orig.rename({'thetao':'theta_ocean'}), file_S_orig.rename({'so':'salinity_ocean'})]).sel(Nisf=rignot_isf).assign_coords({'time': tt})\n",
    "        file_TS_list.append(file_TS_orig)\n",
    "    file_TS = xr.concat(file_TS_list, dim='time').rename({'z':'depth'})\n",
    "    file_TS['depth'] = -1*file_TS['depth']\n",
    "    depth_axis_old = file_TS.depth.values\n",
    "    depth_axis_new = np.concatenate((np.zeros(1),depth_axis_old))\n",
    "    file_TS_with_shallow = file_TS.interp({'depth': depth_axis_new})\n",
    "    filled_TS = file_TS_with_shallow.ffill(dim='depth').bfill(dim='depth').sel(Nisf=file_isf.Nisf) #, 'profile_domain': 1})\n",
    "\n",
    "    print('Prepare input variables')\n",
    "    n = 0\n",
    "    for kisf in tqdm(file_isf.Nisf):\n",
    "        depth_kisf = uf.choose_isf(ice_draft_pos,isf_stack_mask, kisf)\n",
    "        depth_of_int0 = depth_kisf.where(depth_kisf < file_isf['front_bot_depth_max'].sel(Nisf=kisf), \n",
    "                                       file_isf['front_bot_depth_max'].sel(Nisf=kisf))\n",
    "        depth_of_int = depth_of_int0.where(depth_kisf > file_isf['front_ice_depth_min'].sel(Nisf=kisf), \n",
    "                                       file_isf['front_ice_depth_min'].sel(Nisf=kisf))\n",
    "\n",
    "        T_isf = filled_TS['theta_ocean'].sel(Nisf=kisf).interp({'depth': depth_of_int}).drop('depth')\n",
    "        S_isf = filled_TS['salinity_ocean'].sel(Nisf=kisf).interp({'depth': depth_of_int}).drop('depth')\n",
    "\n",
    "        cell_area_kisf = uf.choose_isf(cell_area_weight ,isf_stack_mask, kisf) \n",
    "        isf_conc_kisf = uf.choose_isf(file_isf_conc,isf_stack_mask, kisf) \n",
    "        weight_kisf = cell_area_kisf * isf_conc_kisf\n",
    "\n",
    "        T_mean_cav = uf.weighted_mean(T_isf, 'mask_coord', weight_kisf).to_dataset(name='T_mean')\n",
    "        S_mean_cav = uf.weighted_mean(S_isf, 'mask_coord', weight_kisf).to_dataset(name='S_mean')\n",
    "        T_std_cav = uf.weighted_std(T_isf, 'mask_coord', weight_kisf).to_dataset(name='T_std')\n",
    "        S_std_cav = uf.weighted_std(S_isf, 'mask_coord', weight_kisf).to_dataset(name='S_std')\n",
    "        T_S_2D_meanstd_kisf = xr.merge([T_mean_cav,S_mean_cav,T_std_cav,S_std_cav])\n",
    "\n",
    "        T_S_info_br, TSmean_br = xr.broadcast(xr.merge([T_isf.rename('theta_in'),S_isf.rename('salinity_in')]),T_S_2D_meanstd_kisf)\n",
    "        TS_info_all = xr.merge([T_S_info_br, TSmean_br])\n",
    "\n",
    "        file_isf_kisf = uf.choose_isf(file_isf[['dGL', 'dIF']], isf_stack_mask, kisf)\n",
    "        bathy_kisf = uf.choose_isf(file_bed_goodGL, isf_stack_mask, kisf)\n",
    "        slope_kisf = uf.choose_isf(file_slope, isf_stack_mask, kisf)\n",
    "        geometry_kisf = xr.merge([file_isf_kisf,\n",
    "                             depth_kisf.rename('corrected_isfdraft'),\n",
    "                             bathy_kisf.rename('bathy_metry'),\n",
    "                             slope_kisf])\n",
    "\n",
    "\n",
    "        geometry_2D_br, time_dpdt_in_br = xr.broadcast(geometry_kisf,TS_info_all)\n",
    "        all_info = xr.merge([geometry_2D_br, time_dpdt_in_br])\n",
    "\n",
    "        if n == 0:\n",
    "            all_info_all = all_info.squeeze().drop('Nisf')\n",
    "        else:\n",
    "            all_info_all =  all_info_all.combine_first(all_info).squeeze().drop('Nisf')\n",
    "        n = n+1        \n",
    "\n",
    "    for mod_size in ['xsmall96','small','large']: #,\n",
    "\n",
    "        print('NN size',mod_size)\n",
    "        seed_nb = 1\n",
    "        model = keras.models.load_model(path_model + 'model_nn_'+mod_size+'_'+exp_name+'_wholedataset_'+str(seed_nb).zfill(2)+'_TS'+TS_opt+'_norm'+norm_method+'.h5')\n",
    "\n",
    "        print('Computing 2D patterns')\n",
    "        xr_ensmean_res2D = apply_NN_results_2D_allisf_allyears(file_isf, norm_metrics, all_info_all, model, input_vars)\n",
    "\n",
    "        print('Compute the 1D evalmetrics')\n",
    "        res_1D_list = []\n",
    "        for kisf in tqdm(file_isf.Nisf.values): \n",
    "\n",
    "            geometry_isf_2D = dfmt.choose_isf(geometry_info_2D,isf_stack_mask, kisf)\n",
    "            melt_rate_2D_isf_m_per_y = dfmt.choose_isf(xr_ensmean_res2D,isf_stack_mask, kisf)\n",
    "\n",
    "            melt_rate_1D_isf_Gt_per_y = (melt_rate_2D_isf_m_per_y * geometry_isf_2D['grid_cell_area_weighted']).sum(dim=['mask_coord']) * rho_i / 10**12\n",
    "\n",
    "            box_loc_config_stacked = dfmt.choose_isf(box1, isf_stack_mask, kisf)\n",
    "            param_melt_2D_box1_isf = melt_rate_2D_isf_m_per_y.where(np.isfinite(box_loc_config_stacked))\n",
    "\n",
    "            melt_rate_1D_isf_myr_box1_mean = dfmt.weighted_mean(param_melt_2D_box1_isf,['mask_coord'], geometry_isf_2D['grid_cell_area_weighted'])     \n",
    "\n",
    "            out_1D = xr.concat([melt_rate_1D_isf_Gt_per_y, melt_rate_1D_isf_myr_box1_mean], dim='metrics').assign_coords({'metrics': ['Gt','box1']})\n",
    "            res_1D_list.append(out_1D) \n",
    "\n",
    "        res_1D_tt = xr.concat(res_1D_list, dim='Nisf')\n",
    "\n",
    "        res_1D_tt.rename('predicted_melt').to_netcdf(outputpath_melt + 'evalmetrics_1D_NN'+mod_size+'_'+scenario+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "RUN DEEP ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPLY MODEL\n",
    "\n",
    "input_vars = ['dGL','dIF','corrected_isfdraft','bathy_metry','slope_bed_lon','slope_bed_lat','slope_ice_lon','slope_ice_lat',\n",
    "                'theta_in','salinity_in','T_mean', 'S_mean', 'T_std', 'S_std']\n",
    "\n",
    "tuning_sort = 'new' #'old'\n",
    "\n",
    "### use any model from CV over time\n",
    "if tuning_sort == 'new':\n",
    "    path_model = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/NN_MODELS/'\n",
    "elif tuning_sort == 'old':\n",
    "    path_model = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/experiments/WHOLE/'\n",
    "    \n",
    "scenario = 'ssp585'\n",
    "ano_ISMIP = True\n",
    "ano_NEMO = True\n",
    "\n",
    "### APPLY MODEL\n",
    "\n",
    "def apply_NN_results_2D_allisf_allyears(file_isf, norm_metrics, all_info_all, model, input_vars=[]):\n",
    "    \n",
    "    all_info_df = all_info_all.to_dataframe()\n",
    "    val_norm = pp.normalise_vars(all_info_df[input_vars],\n",
    "                                norm_metrics[input_vars].loc['mean_vars'],\n",
    "                                norm_metrics[input_vars].loc['range_vars'])\n",
    "\n",
    "    x_val_norm = val_norm\n",
    "    \n",
    "    batch_size=4096\n",
    "    y_out_norm = model.predict(x_val_norm.values.astype('float64'),batch_size=batch_size,verbose = 1)\n",
    "\n",
    "    y_out_norm_xr = xr.DataArray(data=y_out_norm.squeeze()).rename({'dim_0': 'index'})\n",
    "    y_out_norm_xr = y_out_norm_xr.assign_coords({'index': x_val_norm.index})\n",
    "\n",
    "    y_out_norm_xr_2D = uf.bring_back_to_2D(y_out_norm_xr)\n",
    "\n",
    "    # denormalise the output\n",
    "    y_out_xr = pp.denormalise_vars(y_out_norm_xr_2D, \n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['mean_vars'],\n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['range_vars'])\n",
    "\n",
    "    y_whole_grid = y_out_xr.reindex_like(file_isf['ISF_mask'])\n",
    "    return y_whole_grid\n",
    "\n",
    "### if we want the \"real\" profiles and not ISMIP + anomalies\n",
    "inputpath_profiles='/bettik/burgardc/DATA/SUMMER_PAPER/interim/T_S_PROF/CMIP/'\n",
    "T_ISMIP = xr.open_dataset(inputpath_profiles + 'T_mean_prof_50km_contshelf_ISMIP.nc')\n",
    "S_ISMIP = xr.open_dataset(inputpath_profiles + 'S_mean_prof_50km_contshelf_ISMIP.nc')\n",
    "TS_ISMIP = xr.merge([T_ISMIP.rename({'thetao':'theta_ocean'}),S_ISMIP.rename({'so':'salinity_ocean'})])\n",
    "    \n",
    "inputpath_profiles_NEMO = '/bettik/burgardc/DATA/BASAL_MELT_PARAM/interim/T_S_PROF/nemo_5km_OPM026/'\n",
    "file_TS_orig = xr.open_dataset(inputpath_profiles_NEMO+'T_S_mean_prof_corrected_km_contshelf_1980-2018_rignotisf.nc')\n",
    "TS_NEMO = file_TS_orig.sel(profile_domain=[50]).squeeze().drop_vars('profile_domain').mean('time')\n",
    "TS_NEMO['depth'] = -1*TS_NEMO['depth']\n",
    "TS_NEMO_rightaxis = TS_NEMO.rename({'depth':'z'}).interp({'z': TS_ISMIP.z})\n",
    "TS_NEMO_withISMIP = xr.concat([TS_NEMO_rightaxis,TS_ISMIP.sel(Nisf=[36,62])], dim='Nisf')\n",
    "\n",
    "### If no ensemble of models\n",
    "for mod in ['ACCESS-CM2','ACCESS-ESM1-5','CESM2','CESM2-WACCM','CNRM-CM6-1','CNRM-ESM2-1',\n",
    "           'CanESM5','GFDL-CM4','GFDL-ESM4','GISS-E2-1-H','IPSL-CM6A-LR','MPI-ESM1-2-HR','MRI-ESM2-0',\n",
    "           'UKESM1-0-LL']: #,,\n",
    "#for mod in ['UKESM1-0-LL']: # continue here\n",
    "            #,,'GFDL-CM4','ACCESS-CM2','ACCESS-ESM1-5','CESM2','CESM2-WACCM','CNRM-CM6-1','CNRM-ESM2-1',\n",
    "          # 'CanESM5','GFDL-ESM4','GISS-E2-1-H','IPSL-CM6A-LR','MPI-ESM1-2-HR',\n",
    "    print(mod)\n",
    "    \n",
    "    if mod in ['CNRM-CM6-1','CNRM-ESM2-1']:\n",
    "        to2300 = False\n",
    "    elif mod in ['GISS-E2-1-H']:\n",
    "        to2300 = True\n",
    "    elif mod in ['ACCESS-CM2','ACCESS-ESM1-5','CESM2-WACCM','CanESM5','IPSL-CM6A-LR','MRI-ESM2-0']:\n",
    "        to2300 = True\n",
    "    elif mod in ['MPI-ESM1-2-HR','GFDL-CM4','GFDL-ESM4']:\n",
    "        to2300 = False\n",
    "    elif mod == 'UKESM1-0-LL':\n",
    "        to2300 = True     \n",
    "    elif mod == 'CESM2':\n",
    "        to2300 = False        \n",
    "\n",
    "    if (geoyear > 2100) and (not to2300):\n",
    "        continue\n",
    "    \n",
    "    if scenario == 'historical':\n",
    "        yystart = 1850 #1980 #1850\n",
    "        yyend = 2014\n",
    "    elif scenario == 'ssp245':\n",
    "        yystart = 2015\n",
    "        yyend = 2100  \n",
    "    else:\n",
    "        if to2300:\n",
    "            yystart = 2015\n",
    "            yyend = 2300\n",
    "            if (geoyear > 2100):\n",
    "                yystart = geoyear\n",
    "        else:\n",
    "            yystart = 2015\n",
    "            yyend = 2100 \n",
    "\n",
    "    inputpath_profiles='/bettik/burgardc/DATA/SUMMER_PAPER/interim/T_S_PROF/CMIP/'+mod+'/'\n",
    "    outputpath_melt = home_path+'/DATA/SUMMER_PAPER/processed/OCEAN_MELT_RATE_CMIP/'+mod+'/'\n",
    "\n",
    "    T_Clim = xr.open_dataset(inputpath_profiles + 'T_mean_prof_50km_contshelf_'+mod+'_clim.nc')\n",
    "    S_Clim = xr.open_dataset(inputpath_profiles + 'S_mean_prof_50km_contshelf_'+mod+'_clim.nc')\n",
    "    TS_Clim = xr.merge([T_Clim.rename({'thetao':'theta_ocean'}),S_Clim.rename({'so':'salinity_ocean'})])\n",
    "            \n",
    "    # PREPARE VARIABLES\n",
    "    file_TS_list = []\n",
    "    for tt in range(yystart,yyend+1):\n",
    "        file_T_orig = xr.open_dataset(inputpath_profiles+'T_mean_prof_50km_contshelf_'+mod+'_'+scenario+'_'+str(tt)+'.nc')\n",
    "        file_S_orig = xr.open_dataset(inputpath_profiles+'S_mean_prof_50km_contshelf_'+mod+'_'+scenario+'_'+str(tt)+'.nc')\n",
    "        file_TS_orig = xr.merge([file_T_orig.rename({'thetao':'theta_ocean'}), file_S_orig.rename({'so':'salinity_ocean'})]).sel(Nisf=rignot_isf).assign_coords({'time': tt})\n",
    "        file_TS_list.append(file_TS_orig)\n",
    "\n",
    "    file_TS_0 = xr.concat(file_TS_list, dim='time')\n",
    "    if ano_ISMIP:\n",
    "        file_TS = (file_TS_0 - TS_ISMIP*0 + TS_Clim*0).rename({'z':'depth'})\n",
    "    elif ano_NEMO:\n",
    "        file_TS = (file_TS_0 - TS_ISMIP + TS_NEMO_withISMIP).rename({'z':'depth'})\n",
    "    else:\n",
    "        ### if we want the \"real\" profiles and not ISMIP + anomalies\n",
    "        file_TS = (file_TS_0 - TS_ISMIP + TS_Clim).rename({'z':'depth'})\n",
    "        ###\n",
    "    \n",
    "    file_TS['depth'] = -1*file_TS['depth']\n",
    "    depth_axis_old = file_TS.depth.values\n",
    "    depth_axis_new = np.concatenate((np.zeros(1),depth_axis_old))\n",
    "    file_TS_with_shallow = file_TS.interp({'depth': depth_axis_new})\n",
    "    filled_TS = file_TS_with_shallow.ffill(dim='depth').bfill(dim='depth').sel(Nisf=file_isf.Nisf) #, 'profile_domain': 1})\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    print('Prepare input variables')\n",
    "    n = 0\n",
    "    for kisf in tqdm(file_isf.Nisf):\n",
    "        depth_kisf = uf.choose_isf(ice_draft_pos,isf_stack_mask, kisf)\n",
    "        depth_of_int0 = depth_kisf.where(depth_kisf < file_isf['front_bot_depth_max'].sel(Nisf=kisf), \n",
    "                                       file_isf['front_bot_depth_max'].sel(Nisf=kisf))\n",
    "        depth_of_int = depth_of_int0.where(depth_kisf > file_isf['front_ice_depth_min'].sel(Nisf=kisf), \n",
    "                                       file_isf['front_ice_depth_min'].sel(Nisf=kisf))\n",
    "\n",
    "        T_isf = filled_TS['theta_ocean'].sel(Nisf=kisf).interp({'depth': depth_of_int}).drop_vars('depth')\n",
    "        S_isf = filled_TS['salinity_ocean'].sel(Nisf=kisf).interp({'depth': depth_of_int}).drop_vars('depth')\n",
    "\n",
    "        cell_area_kisf = uf.choose_isf(cell_area_weight ,isf_stack_mask, kisf) \n",
    "        isf_conc_kisf = uf.choose_isf(file_isf_conc,isf_stack_mask, kisf) \n",
    "        weight_kisf = cell_area_kisf * isf_conc_kisf\n",
    "\n",
    "        T_mean_cav = uf.weighted_mean(T_isf, 'mask_coord', weight_kisf).to_dataset(name='T_mean')\n",
    "        S_mean_cav = uf.weighted_mean(S_isf, 'mask_coord', weight_kisf).to_dataset(name='S_mean')\n",
    "        T_std_cav = uf.weighted_std(T_isf, 'mask_coord', weight_kisf).to_dataset(name='T_std')\n",
    "        S_std_cav = uf.weighted_std(S_isf, 'mask_coord', weight_kisf).to_dataset(name='S_std')\n",
    "        T_S_2D_meanstd_kisf = xr.merge([T_mean_cav,S_mean_cav,T_std_cav,S_std_cav])\n",
    "\n",
    "        T_S_info_br, TSmean_br = xr.broadcast(xr.merge([T_isf.rename('theta_in'),S_isf.rename('salinity_in')]),T_S_2D_meanstd_kisf)\n",
    "        TS_info_all = xr.merge([T_S_info_br, TSmean_br])\n",
    "\n",
    "        file_isf_kisf = uf.choose_isf(file_isf[['dGL', 'dIF']], isf_stack_mask, kisf)\n",
    "        bathy_kisf = uf.choose_isf(file_bed_goodGL, isf_stack_mask, kisf)\n",
    "        slope_kisf = uf.choose_isf(file_slope, isf_stack_mask, kisf)\n",
    "        geometry_kisf = xr.merge([file_isf_kisf,\n",
    "                             depth_kisf.rename('corrected_isfdraft'),\n",
    "                             bathy_kisf.rename('bathy_metry'),\n",
    "                             slope_kisf])\n",
    "\n",
    "\n",
    "        geometry_2D_br, time_dpdt_in_br = xr.broadcast(geometry_kisf,TS_info_all)\n",
    "        all_info = xr.merge([geometry_2D_br, time_dpdt_in_br])\n",
    "\n",
    "        if n == 0:\n",
    "            all_info_all = all_info.squeeze().drop_vars('Nisf')\n",
    "        else:\n",
    "            all_info_all =  all_info_all.combine_first(all_info).squeeze().drop_vars('Nisf')\n",
    "        n = n+1        \n",
    "\n",
    "    mod_size = 'small'\n",
    "    \n",
    "    res_2D_list = []\n",
    "    res2D_sum = 0\n",
    "    for seed_nb in range(1,11):\n",
    "        print('seed_nb',seed_nb)\n",
    "\n",
    "        model = keras.models.load_model(path_model + 'model_nn_'+mod_size+'_'+exp_name+'_wholedataset_'+str(seed_nb).zfill(2)+'_TS'+TS_opt+'_norm'+norm_method+'.h5', \n",
    "                                        compile=False)\n",
    "        model.compile(optimizer=\"adam\",loss=tf.keras.losses.MeanSquaredError(),metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        \n",
    "        print('Computing 2D patterns')\n",
    "        res2D = apply_NN_results_2D_allisf_allyears(file_isf, norm_metrics, all_info_all, model, input_vars)\n",
    "        res2D_sum = res2D_sum + res2D\n",
    "        #res_2D_list.append(res2D.assign_coords({'seed_nb': seed_nb}).chunk({'time': 20}))\n",
    "        \n",
    "    #res2D_all = xr.concat(res_2D_list, dim='seed_nb')\n",
    "    #xr_ensmean_res2D = res2D_all.mean('seed_nb').load()\n",
    "    xr_ensmean_res2D = res2D_sum / 10\n",
    "    #del res2D_all\n",
    "    \n",
    "    print('Compute the 1D evalmetrics')\n",
    "    res_1D_list = []\n",
    "    for kisf in tqdm(file_isf.Nisf.values): \n",
    "\n",
    "        geometry_isf_2D = dfmt.choose_isf(geometry_info_2D,isf_stack_mask, kisf)\n",
    "        melt_rate_2D_isf_m_per_y = dfmt.choose_isf(xr_ensmean_res2D,isf_stack_mask, kisf)\n",
    "\n",
    "        melt_rate_1D_isf_Gt_per_y = (melt_rate_2D_isf_m_per_y * geometry_isf_2D['grid_cell_area_weighted']).sum(dim=['mask_coord']) * rho_i / 10**12\n",
    "\n",
    "        box_loc_config_stacked = dfmt.choose_isf(box1, isf_stack_mask, kisf)\n",
    "        param_melt_2D_box1_isf = melt_rate_2D_isf_m_per_y.where(np.isfinite(box_loc_config_stacked))\n",
    "\n",
    "        melt_rate_1D_isf_myr_box1_mean = dfmt.weighted_mean(param_melt_2D_box1_isf,['mask_coord'], geometry_isf_2D['grid_cell_area_weighted'])     \n",
    "\n",
    "        out_1D = xr.concat([melt_rate_1D_isf_Gt_per_y, melt_rate_1D_isf_myr_box1_mean], dim='metrics').assign_coords({'metrics': ['Gt','box1']})\n",
    "        res_1D_list.append(out_1D) \n",
    "\n",
    "    res_1D_tt = xr.concat(res_1D_list, dim='Nisf')\n",
    "\n",
    "    if ano_ISMIP:\n",
    "        res_1D_tt.rename('predicted_melt').to_netcdf(outputpath_melt + 'evalmetrics_1D_NN'+mod_size+'_'+scenario+'_anoISMIP_ElmerIcegeo'+str(geoyear)+'.nc')\n",
    "    elif ano_NEMO:\n",
    "        res_1D_tt.rename('predicted_melt').to_netcdf(outputpath_melt + 'evalmetrics_1D_NN'+mod_size+'_'+scenario+'_anoNEMO_ElmerIcegeo'+str(geoyear)+'.nc')\n",
    "    else:\n",
    "        res_1D_tt.rename('predicted_melt').to_netcdf(outputpath_melt + 'evalmetrics_1D_NN'+mod_size+'_'+scenario+'_notISMIP_ElmerIcegeo'+str(geoyear)+'.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoyear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "################## DO NOT USE ANYMORE!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_size = 'xsmall96'\n",
    "\n",
    "### APPLY MODEL\n",
    "\n",
    "input_vars = ['dGL','dIF','corrected_isfdraft','bathy_metry','slope_bed_lon','slope_bed_lat','slope_ice_lon','slope_ice_lat',\n",
    "                'theta_in','salinity_in','T_mean', 'S_mean', 'T_std', 'S_std']\n",
    "\n",
    "tuning_sort = 'new' #'old'\n",
    "\n",
    "### use any model from CV over time\n",
    "outputpath_melt = home_path+'/DATA/SUMMER_PAPER/processed/OCEAN_MELT_RATE_CMIP/'+mod+'/'\n",
    "if tuning_sort == 'new':\n",
    "    path_model = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/NN_MODELS/'\n",
    "elif tuning_sort == 'old':\n",
    "    path_model = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/experiments/WHOLE/'\n",
    "\n",
    "def apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_nrun, model, input_vars=[]):\n",
    "    \"\"\"\n",
    "    Compute 2D melt based on a given NN model\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    val_norm = pp.normalise_vars(df_nrun[input_vars],\n",
    "                                norm_metrics[input_vars].loc['mean_vars'],\n",
    "                                norm_metrics[input_vars].loc['range_vars'])\n",
    "\n",
    "    x_val_norm = val_norm\n",
    "\n",
    "    y_out_norm = model.predict(x_val_norm.values.astype('float64'),verbose = 0)\n",
    "\n",
    "    y_out_norm_xr = xr.DataArray(data=y_out_norm.squeeze()).rename({'dim_0': 'index'})\n",
    "    y_out_norm_xr = y_out_norm_xr.assign_coords({'index': x_val_norm.index})\n",
    "\n",
    "    # denormalise the output\n",
    "    y_out = pp.denormalise_vars(y_out_norm_xr, \n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['mean_vars'],\n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['range_vars'])\n",
    "\n",
    "    y_out_pd_s = pd.Series(y_out.values,index=df_nrun.index,name='predicted_melt') \n",
    "\n",
    "    # put some order in the file\n",
    "    y_out_xr = y_out_pd_s.to_xarray().sortby('y')\n",
    "\n",
    "    y_whole_grid = y_out_xr.reindex_like(file_isf['ISF_mask'])\n",
    "    return y_whole_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('NN size',mod_size)\n",
    "    seed_nb = 1\n",
    "    model = keras.models.load_model(path_model + 'model_nn_'+mod_size+'_'+exp_name+'_wholedataset_'+str(seed_nb).zfill(2)+'_TS'+TS_opt+'_norm'+norm_method+'.h5')\n",
    "    \n",
    "    if scenario == 'historical':\n",
    "        print('prepare 2D input')\n",
    "        li = []\n",
    "        for tt in tqdm(range(yystart,yyend+1)):    \n",
    "            #print('Combining all dataframes')\n",
    "        \n",
    "            df_nrun = pd.read_csv(inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt)+'.csv',index_col=[0,1,2])\n",
    "            li.append(df_nrun)\n",
    "\n",
    "        print('Concatenating input')\n",
    "        df_allyy = pd.concat(li)\n",
    "        \n",
    "        print('Computing 2D patterns')\n",
    "        xr_ensmean_res2D = apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_allyy, model, input_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### APPLY MODEL\n",
    "\n",
    "input_vars = ['dGL','dIF','corrected_isfdraft','bathy_metry','slope_bed_lon','slope_bed_lat','slope_ice_lon','slope_ice_lat',\n",
    "                'theta_in','salinity_in','T_mean', 'S_mean', 'T_std', 'S_std']\n",
    "\n",
    "tuning_sort = 'new' #'old'\n",
    "\n",
    "### use any model from CV over time\n",
    "outputpath_melt = home_path+'/DATA/SUMMER_PAPER/processed/OCEAN_MELT_RATE_CMIP/'+mod+'/'\n",
    "if tuning_sort == 'new':\n",
    "    path_model = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/NN_MODELS/'\n",
    "elif tuning_sort == 'old':\n",
    "    path_model = '/bettik/burgardc/DATA/NN_PARAM/interim/NN_MODELS/experiments/WHOLE/'\n",
    "\n",
    "def apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_nrun, model, input_vars=[]):\n",
    "    \"\"\"\n",
    "    Compute 2D melt based on a given NN model\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    val_norm = pp.normalise_vars(df_nrun[input_vars],\n",
    "                                norm_metrics[input_vars].loc['mean_vars'],\n",
    "                                norm_metrics[input_vars].loc['range_vars'])\n",
    "\n",
    "    x_val_norm = val_norm\n",
    "\n",
    "    y_out_norm = model.predict(x_val_norm.values.astype('float64'),verbose = 0)\n",
    "\n",
    "    y_out_norm_xr = xr.DataArray(data=y_out_norm.squeeze()).rename({'dim_0': 'index'})\n",
    "    y_out_norm_xr = y_out_norm_xr.assign_coords({'index': x_val_norm.index})\n",
    "\n",
    "    # denormalise the output\n",
    "    y_out = pp.denormalise_vars(y_out_norm_xr, \n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['mean_vars'],\n",
    "                             norm_metrics['melt_m_ice_per_y'].loc['range_vars'])\n",
    "\n",
    "    y_out_pd_s = pd.Series(y_out.values,index=df_nrun.index,name='predicted_melt') \n",
    "\n",
    "    # put some order in the file\n",
    "    y_out_xr = y_out_pd_s.to_xarray().sortby('y')\n",
    "\n",
    "    y_whole_grid = y_out_xr.reindex_like(file_isf['ISF_mask'])\n",
    "    return y_whole_grid\n",
    "\n",
    "### If no ensemble of models\n",
    "\n",
    "for mod_size in ['xsmall96','small','large']: #\n",
    "    \n",
    "    print('NN size',mod_size)\n",
    "    seed_nb = 1\n",
    "    model = keras.models.load_model(path_model + 'model_nn_'+mod_size+'_'+exp_name+'_wholedataset_'+str(seed_nb).zfill(2)+'_TS'+TS_opt+'_norm'+norm_method+'.h5')\n",
    "    \n",
    "    if scenario == 'historical':\n",
    "        print('prepare 2D input')\n",
    "        li = []\n",
    "        for tt in tqdm(range(yystart,yyend+1)):    \n",
    "            #print('Combining all dataframes')\n",
    "        \n",
    "            df_nrun = pd.read_csv(inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt)+'.csv',index_col=[0,1,2])\n",
    "            li.append(df_nrun)\n",
    "\n",
    "        print('Concatenating input')\n",
    "        df_allyy = pd.concat(li)\n",
    "        \n",
    "        print('Computing 2D patterns')\n",
    "        xr_ensmean_res2D = apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_allyy, model, input_vars)\n",
    "\n",
    "        print('Compute the 1D evalmetrics')\n",
    "        res_1D_list = []\n",
    "        for kisf in tqdm(file_isf.Nisf.values): \n",
    "\n",
    "            geometry_isf_2D = dfmt.choose_isf(geometry_info_2D,isf_stack_mask, kisf)\n",
    "            melt_rate_2D_isf_m_per_y = dfmt.choose_isf(xr_ensmean_res2D,isf_stack_mask, kisf)\n",
    "\n",
    "            melt_rate_1D_isf_Gt_per_y = (melt_rate_2D_isf_m_per_y * geometry_isf_2D['grid_cell_area_weighted']).sum(dim=['mask_coord']) * rho_i / 10**12\n",
    "\n",
    "            box_loc_config_stacked = dfmt.choose_isf(box1, isf_stack_mask, kisf)\n",
    "            param_melt_2D_box1_isf = melt_rate_2D_isf_m_per_y.where(np.isfinite(box_loc_config_stacked))\n",
    "\n",
    "            melt_rate_1D_isf_myr_box1_mean = dfmt.weighted_mean(param_melt_2D_box1_isf,['mask_coord'], geometry_isf_2D['grid_cell_area_weighted'])     \n",
    "\n",
    "            out_1D = xr.concat([melt_rate_1D_isf_Gt_per_y, melt_rate_1D_isf_myr_box1_mean], dim='metrics').assign_coords({'metrics': ['Gt','box1']})\n",
    "            res_1D_list.append(out_1D) \n",
    "\n",
    "        res_1D_all = xr.concat(res_1D_list, dim='Nisf')\n",
    "        res_1D_all.rename('predicted_melt').to_netcdf(outputpath_melt + 'evalmetrics_1D_NN'+mod_size+'_'+scenario+'.nc')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        res_1Dtt_list = []\n",
    "        \n",
    "        for tt in tqdm(range(yystart,yyend+1,50)):    \n",
    "            #print('Combining all dataframes')\n",
    "            trange = range(tt,tt+50)\n",
    "            \n",
    "            li = []\n",
    "            for tt0 in trange: \n",
    "                \n",
    "                if os.path.exists(inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt0)+'.csv'):\n",
    "                    df_nrun = pd.read_csv(inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt0)+'.csv',index_col=[0,1,2])\n",
    "                    li.append(df_nrun)  \n",
    "                    ttend = tt0\n",
    "                else:\n",
    "                    print(str(tt0)+' file does not exist')\n",
    "                \n",
    "            trange = range(tt,ttend+1)\n",
    "            df_allyy = pd.concat(li)\n",
    "\n",
    "            #print('Computing 2D patterns')\n",
    "            xr_ensmean_res2D = apply_NN_results_2D_1isf_1year(file_isf, norm_metrics, df_allyy, model, input_vars)\n",
    "\n",
    "            #print('Compute the 1D evalmetrics')\n",
    "            res_1D_list = []\n",
    "            for kisf in file_isf.Nisf.values: \n",
    "\n",
    "                geometry_isf_2D = dfmt.choose_isf(geometry_info_2D,isf_stack_mask, kisf)\n",
    "                melt_rate_2D_isf_m_per_y = dfmt.choose_isf(xr_ensmean_res2D,isf_stack_mask, kisf)\n",
    "\n",
    "                melt_rate_1D_isf_Gt_per_y = (melt_rate_2D_isf_m_per_y * geometry_isf_2D['grid_cell_area_weighted']).sum(dim=['mask_coord']) * rho_i / 10**12\n",
    "\n",
    "                box_loc_config_stacked = dfmt.choose_isf(box1, isf_stack_mask, kisf)\n",
    "                param_melt_2D_box1_isf = melt_rate_2D_isf_m_per_y.where(np.isfinite(box_loc_config_stacked))\n",
    "\n",
    "                melt_rate_1D_isf_myr_box1_mean = dfmt.weighted_mean(param_melt_2D_box1_isf,['mask_coord'], geometry_isf_2D['grid_cell_area_weighted'])     \n",
    "\n",
    "                out_1D = xr.concat([melt_rate_1D_isf_Gt_per_y, melt_rate_1D_isf_myr_box1_mean], dim='metrics').assign_coords({'metrics': ['Gt','box1']})\n",
    "                res_1D_list.append(out_1D) \n",
    "\n",
    "            res_1D_tt = xr.concat(res_1D_list, dim='Nisf').assign_coords({'time': trange})\n",
    "            res_1Dtt_list.append(res_1D_tt)\n",
    "            \n",
    "        res_1D_all = xr.concat(res_1Dtt_list, dim='time')\n",
    "        res_1D_all.rename('predicted_melt').to_netcdf(outputpath_melt + 'evalmetrics_1D_NN'+mod_size+'_'+scenario+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt)+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt0)+'.csv'):\n",
    "    print('True')\n",
    "else:\n",
    "    print('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_csv + 'dataframe_input_allisf_'+mod+'_'+scenario+'_'+str(tt0)+'.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "###############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
