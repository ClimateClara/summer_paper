{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a498da6-a904-490f-9225-c5e69c937927",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Sep 08 11:15 2022\n",
    "\n",
    "This script is to prepare the normalising coefficients and input data for the training on the whole dataset\n",
    "\n",
    "Author: Clara Burgard\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62683553-5f6d-4aff-bc72-48afd2a44509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "from tqdm import trange, tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from multimelt.constants import *\n",
    "import summer_paper.data_formatting_NN as dfmt\n",
    "import summer_paper.prep_input_data_NN as indat\n",
    "\n",
    "import distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f53473f-448b-4330-83cc-38f8f2e56973",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client(n_workers=8, dashboard_address=':8795', local_directory='/tmp', memory_limit='4GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d8e89-2eda-46fc-8660-bf0ed28cf632",
   "metadata": {},
   "source": [
    "PREPARE THE CONTEXT OF THE INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3457f121-607f-4f0e-8941-b59bc0cd8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/' \n",
    "\n",
    "tblock_dim = np.arange(1,14).tolist()+np.arange(21,50).tolist()\n",
    "isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b4415-3359-44ce-841e-a3d1f89defd3",
   "metadata": {},
   "source": [
    "prepare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b446c7f-4078-45f3-82b2-17322ccf60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_per_timeblock(tt, isf_dim, TS_opt, inputpath_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Combines all csv of ice shelves of one time block\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tblock_dim : list\n",
    "        List of all time blocks to conduct the cross-validation on.\n",
    "    isf_dim : list\n",
    "        List of all ice shelves to conduct the cross-validation on.\n",
    "    tblock_out : int\n",
    "        Time block to leave out in cross-validation.\n",
    "    isf_out : list\n",
    "        Ice shelf to leave out in cross-validation.\n",
    "    TS_opt : str\n",
    "        Type of input temperature and salinity profiles to use. Can be 'extrap', 'whole', 'thermocline'\n",
    "    inputpath_data : str\n",
    "        Path to folder where to find the preformatted csv files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summary_ds_all: xr.Dataset\n",
    "        Dataset containing mean and denominator of the normalisation.\n",
    "    var_train_norm: xr.Dataset\n",
    "        Dataset containing normalised training predictors and target.\n",
    "    var_val_norm: xr.Dataset\n",
    "        Dataset containing normalised validation predictors and target.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## which profile option are we using for temperature and salinity\n",
    "    if TS_opt == 'extrap':\n",
    "        inputpath_prof = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "    elif TS_opt == 'whole':\n",
    "        inputpath_prof = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "    elif TS_opt == 'thermocline':\n",
    "        inputpath_prof = inputpath_data+'THERMOCLINE_CHUNKS/'\n",
    "\n",
    "    ### prepare training dataset\n",
    "\n",
    "    train_input_df = None        \n",
    "\n",
    "    for kisf in tqdm(isf_dim): \n",
    "\n",
    "        #print(kisf)\n",
    "        clean_df_nrun_kisf = pd.read_csv(inputpath_prof + 'dataframe_input_isf'+str(kisf).zfill(3)+'_'+str(tt).zfill(3)+'_new.csv',index_col=[0,1,2])\n",
    "        if 'profile_domain' in clean_df_nrun_kisf.columns:\n",
    "            clean_df_nrun_kisf = clean_df_nrun_kisf.drop(['profile_domain'], axis=1)\n",
    "        clean_df_nrun_kisf.reset_index(drop=True, inplace=True)\n",
    "        #print('here1')\n",
    "        clean_ds_nrun_kisf = clean_df_nrun_kisf.to_xarray()\n",
    "\n",
    "        #print('here2')\n",
    "        if train_input_df is None:\n",
    "            train_input_df = clean_ds_nrun_kisf.copy()\n",
    "        else:\n",
    "            new_index = clean_ds_nrun_kisf.index.values + train_input_df.index.max().values+1\n",
    "            clean_ds_nrun_kisf = clean_ds_nrun_kisf.assign_coords({'index': new_index})\n",
    "            train_input_df = xr.concat([train_input_df, clean_ds_nrun_kisf], dim='index') \n",
    "    \n",
    "    return train_input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab1709d-4072-4f62-add9-78eb920765d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tblock_dim = np.arange(1,14).tolist()+np.arange(21,50).tolist()\n",
    "#tblock_dim = np.arange(21,50).tolist()\n",
    "isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]\n",
    "TS_opt = 'extrap'\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    outputpath_CVinput = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "elif TS_opt == 'whole':\n",
    "    outputpath_CVinput = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "elif TS_opt == 'thermocline':\n",
    "    outputpath_CVinput = inputpath_data+'THERMOCLINE_CHUNKS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9032de01-caca-4390-afd0-e4fb9b2e3ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'combine_csv_per_timeblock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m41\u001b[39m,\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tt)\n\u001b[0;32m----> 5\u001b[0m     var_train \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_csv_per_timeblock\u001b[49m(tt, isf_dim, TS_opt, inputpath_data)\n\u001b[1;32m      6\u001b[0m     var_train\u001b[38;5;241m.\u001b[39mto_netcdf(outputpath_CVinput \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataframe_input_allisf_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(tt)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combine_csv_per_timeblock' is not defined"
     ]
    }
   ],
   "source": [
    "#for tt in tblock_dim:\n",
    "for tt in range(41,50):\n",
    "    print(tt)\n",
    "        \n",
    "    var_train = combine_csv_per_timeblock(tt, isf_dim, TS_opt, inputpath_data)\n",
    "    var_train.to_netcdf(outputpath_CVinput + 'dataframe_input_allisf_'+str(tt).zfill(3)+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f26199-d178-4bc0-9bf9-c23fdd7d9464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b204bbdeaf97461c83227db396bb7945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tt_list = []\n",
    "max_idx_prev = 0\n",
    "for tt in tqdm(tblock_dim):\n",
    "    df_tt = xr.open_mfdataset(outputpath_CVinput + 'dataframe_input_allisf_'+str(tt).zfill(3)+'.nc')\n",
    "    if tt > 1:\n",
    "        df_tt = df_tt.assign_coords({'index': (df_tt.index + max_idx_prev + 1).astype('int64')})\n",
    "    max_idx_prev = df_tt.index.max()\n",
    "    df_tt_list.append(df_tt.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70522397-04f1-465d-b801-42849a3cf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_all = xr.concat(df_tt_list, dim='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fa0b75d-afea-4bce-8c8f-92e2bd9f5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_all.to_netcdf(outputpath_CVinput + 'dataframe_input_allisf_timeblocks_training_summerpaper_notnormed_new.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16b4f21-9159-42c5-8df3-5fb0078af5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tt_all_mean = df_tt_all.mean('index')\n",
    "df_tt_all_std = df_tt_all.std('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f1d1799-5d57-4dae-9182-1b63c9599e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_method = 'std'\n",
    "norm_mean = df_tt_all_mean.assign_coords({'metric': 'mean_vars', 'norm_method': norm_method})\n",
    "norm_range = df_tt_all_std.assign_coords({'metric': 'range_vars', 'norm_method': norm_method})\n",
    "summary_metrics = xr.concat([norm_mean, norm_range], dim='metric').assign_coords({'norm_method': norm_method})\n",
    "summary_metrics.to_netcdf(outputpath_CVinput + 'metrics_norm_wholedataset_origexcept26_christoph_new.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b344c996-be75-499a-9ec6-787099a62a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_mean = summary_metrics.sel(metric='mean_vars')\n",
    "var_range = summary_metrics.sel(metric='range_vars')\n",
    "\n",
    "var_train_norm = (df_tt_all - var_mean)/var_range\n",
    "var_train_norm.to_netcdf(outputpath_CVinput + 'train_data_wholedataset_origexcept26_christoph.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd960f4-578d-43ec-af71-ee8603dee1be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
