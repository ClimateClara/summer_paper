{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a498da6-a904-490f-9225-c5e69c937927",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Sep 08 11:15 2022\n",
    "\n",
    "This script is to prepare the normalising coefficients and input data for the training on the whole dataset\n",
    "\n",
    "Author: Clara Burgard\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62683553-5f6d-4aff-bc72-48afd2a44509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import trange, tqdm\n",
    "#from tqdm import trange, tqdm\n",
    "import glob\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from basal_melt_neural_networks.constants import *\n",
    "import basal_melt_neural_networks.diagnostic_functions as diag\n",
    "import basal_melt_neural_networks.data_formatting as dfmt\n",
    "import basal_melt_neural_networks.prep_input_data as indat\n",
    "\n",
    "import distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53473f-448b-4330-83cc-38f8f2e56973",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client(n_workers=4, dashboard_address=':8795', local_directory='/tmp', memory_limit='6GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d8e89-2eda-46fc-8660-bf0ed28cf632",
   "metadata": {},
   "source": [
    "PREPARE THE CONTEXT OF THE INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457f121-607f-4f0e-8941-b59bc0cd8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data = '/bettik/burgardc/DATA/NN_PARAM/interim/INPUT_DATA/' \n",
    "\n",
    "tblock_dim = np.arange(1,17).tolist()+np.arange(21,50).tolist()\n",
    "isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b4415-3359-44ce-841e-a3d1f89defd3",
   "metadata": {},
   "source": [
    "prepare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b446c7f-4078-45f3-82b2-17322ccf60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_per_timeblock(tt, isf_dim, TS_opt, inputpath_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Combines all csv of ice shelves of one time block\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tblock_dim : list\n",
    "        List of all time blocks to conduct the cross-validation on.\n",
    "    isf_dim : list\n",
    "        List of all ice shelves to conduct the cross-validation on.\n",
    "    tblock_out : int\n",
    "        Time block to leave out in cross-validation.\n",
    "    isf_out : list\n",
    "        Ice shelf to leave out in cross-validation.\n",
    "    TS_opt : str\n",
    "        Type of input temperature and salinity profiles to use. Can be 'extrap', 'whole', 'thermocline'\n",
    "    inputpath_data : str\n",
    "        Path to folder where to find the preformatted csv files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summary_ds_all: xr.Dataset\n",
    "        Dataset containing mean and denominator of the normalisation.\n",
    "    var_train_norm: xr.Dataset\n",
    "        Dataset containing normalised training predictors and target.\n",
    "    var_val_norm: xr.Dataset\n",
    "        Dataset containing normalised validation predictors and target.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## which profile option are we using for temperature and salinity\n",
    "    if TS_opt == 'extrap':\n",
    "        inputpath_prof = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "    elif TS_opt == 'whole':\n",
    "        inputpath_prof = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "    elif TS_opt == 'thermocline':\n",
    "        inputpath_prof = inputpath_data+'THERMOCLINE_CHUNKS/'\n",
    "\n",
    "    ### prepare training dataset\n",
    "\n",
    "    train_input_df = None        \n",
    "\n",
    "    for kisf in tqdm(isf_dim): \n",
    "\n",
    "        #print(kisf)\n",
    "        clean_df_nrun_kisf = pd.read_csv(inputpath_prof + 'dataframe_input_isf'+str(kisf).zfill(3)+'_'+str(tt).zfill(3)+'.csv',index_col=[0,1,2])\n",
    "        clean_df_nrun_kisf.reset_index(drop=True, inplace=True)\n",
    "        #print('here1')\n",
    "        clean_ds_nrun_kisf = clean_df_nrun_kisf.to_xarray()\n",
    "\n",
    "        #print('here2')\n",
    "        if train_input_df is None:\n",
    "            train_input_df = clean_ds_nrun_kisf.copy()\n",
    "        else:\n",
    "            new_index = clean_ds_nrun_kisf.index.values + train_input_df.index.max().values+1\n",
    "            clean_ds_nrun_kisf = clean_ds_nrun_kisf.assign_coords({'index': new_index})\n",
    "            train_input_df = xr.concat([train_input_df, clean_ds_nrun_kisf], dim='index') \n",
    "    \n",
    "    return train_input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121b0cb-4de6-4949-bf08-ca3c8f59214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tblock_dim = np.arange(1,17).tolist()+np.arange(21,50).tolist()\n",
    "tblock_dim = np.arange(21,50).tolist()\n",
    "isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]\n",
    "TS_opt = 'extrap'\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    outputpath_CVinput = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "elif TS_opt == 'whole':\n",
    "    outputpath_CVinput = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "elif TS_opt == 'thermocline':\n",
    "    outputpath_CVinput = inputpath_data+'THERMOCLINE_CHUNKS/'\n",
    "\n",
    "for tt in tblock_dim:\n",
    "    print(tt)\n",
    "        \n",
    "    var_train = combine_csv_per_timeblock(tt, isf_dim, TS_opt, inputpath_data)\n",
    "    var_train.to_netcdf(outputpath_CVinput + 'dataframe_input_allisf_'+str(tt).zfill(3)+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc98a3f-5916-4113-a820-c364b2645b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data_whole_summer_paper(tblock_dim, isf_dim, tblock_out, isf_out, TS_opt, inputpath_data):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes normalisation metrics and normalised predictors and target for cross-validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tblock_dim : list\n",
    "        List of all time blocks to conduct the cross-validation on.\n",
    "    isf_dim : list\n",
    "        List of all ice shelves to conduct the cross-validation on.\n",
    "    tblock_out : int\n",
    "        Time block to leave out in cross-validation.\n",
    "    isf_out : list\n",
    "        Ice shelf to leave out in cross-validation.\n",
    "    TS_opt : str\n",
    "        Type of input temperature and salinity profiles to use. Can be 'extrap', 'whole', 'thermocline'\n",
    "    inputpath_data : str\n",
    "        Path to folder where to find the preformatted csv files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summary_ds_all: xr.Dataset\n",
    "        Dataset containing mean and denominator of the normalisation.\n",
    "    var_train_norm: xr.Dataset\n",
    "        Dataset containing normalised training predictors and target.\n",
    "    var_val_norm: xr.Dataset\n",
    "        Dataset containing normalised validation predictors and target.\n",
    "    \"\"\"\n",
    "\n",
    "    ## are we dealing with leave-one-out cross-validation over time blocks?\n",
    "    tblock_list = list(tblock_dim)\n",
    "    if tblock_out > 0:\n",
    "        tblock_list.remove(tblock_out)\n",
    "    #print(tblock_list)\n",
    "\n",
    "    ## are we dealing with leave-one-out cross-validation over ice shelves?\n",
    "    isf_list = list(isf_dim)\n",
    "    if isf_out > 0:\n",
    "        isf_list.remove(isf_out)\n",
    "    \n",
    "    ## which profile option are we using for temperature and salinity\n",
    "    if TS_opt == 'extrap':\n",
    "        inputpath_prof = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "    elif TS_opt == 'whole':\n",
    "        inputpath_prof = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "    elif TS_opt == 'thermocline':\n",
    "        inputpath_prof = inputpath_data+'THERMOCLINE_CHUNKS/'\n",
    "\n",
    "    ### prepare training dataset\n",
    "\n",
    "    train_input_df = None        \n",
    "\n",
    "    for tt in tblock_list:\n",
    "        print(tt)\n",
    "\n",
    "        for kisf in isf_list: \n",
    "            \n",
    "            #print(kisf)\n",
    "            clean_df_nrun_kisf = pd.read_csv(inputpath_prof + 'dataframe_input_isf'+str(kisf).zfill(3)+'_'+str(tt).zfill(3)+'.csv',index_col=[0,1,2])\n",
    "            clean_df_nrun_kisf.reset_index(drop=True, inplace=True)\n",
    "            #print('here1')\n",
    "            clean_ds_nrun_kisf = clean_df_nrun_kisf.to_xarray()\n",
    "            \n",
    "            #print('here2')\n",
    "            if train_input_df is None:\n",
    "                train_input_df = clean_ds_nrun_kisf.copy()\n",
    "            else:\n",
    "                new_index = clean_ds_nrun_kisf.index.values + train_input_df.index.max().values+1\n",
    "                clean_ds_nrun_kisf = clean_ds_nrun_kisf.assign_coords({'index': new_index})\n",
    "                train_input_df = xr.concat([train_input_df, clean_ds_nrun_kisf], dim='index') \n",
    "        \n",
    "        \n",
    "\n",
    "    ## prepare validation dataset\n",
    "\n",
    "    if (tblock_out > 0) and (isf_out == 0):  \n",
    "        tt_val = [tblock_out]\n",
    "        isf_val = isf_list\n",
    "    elif (isf_out > 0) and (tblock_out == 0):\n",
    "        isf_val = [isf_out]\n",
    "        tt_val = tblock_list\n",
    "    elif (isf_out == 0) and (tblock_out == 0):\n",
    "        isf_val = isf_list\n",
    "        tt_val = tblock_list\n",
    "    else:\n",
    "        print(\"I don't know how to handle leave ice shelves AND time blocks out, please teach me!\")\n",
    "\n",
    "    val_input_df = None        \n",
    "\n",
    "    for tt in tt_val:\n",
    "        print(tt)\n",
    "        \n",
    "        for kisf in isf_val: \n",
    "            \n",
    "            #print(kisf)\n",
    "            clean_df_nrun_kisf = pd.read_csv(inputpath_prof + 'dataframe_input_isf'+str(kisf).zfill(3)+'_'+str(tt).zfill(3)+'.csv',index_col=[0,1,2])\n",
    "            #print('here3')\n",
    "            clean_df_nrun_kisf.reset_index(drop=True, inplace=True)\n",
    "            #print('here4')\n",
    "            clean_ds_nrun_kisf = clean_df_nrun_kisf.to_xarray()\n",
    "\n",
    "            #print('here5')\n",
    "            if val_input_df is None:\n",
    "                val_input_df = clean_ds_nrun_kisf.copy()\n",
    "            else:\n",
    "                new_index = clean_ds_nrun_kisf.index.values + val_input_df.index.max().values+1\n",
    "                clean_ds_nrun_kisf = clean_ds_nrun_kisf.assign_coords({'index': new_index})\n",
    "                val_input_df = xr.concat([val_input_df, clean_ds_nrun_kisf], dim='index') \n",
    "\n",
    "    ## prepare input and target\n",
    "    \n",
    "    print('here6')\n",
    "    y_train = train_input_df['melt_m_ice_per_y']\n",
    "    x_train = train_input_df.drop_vars(['melt_m_ice_per_y'])\n",
    "\n",
    "    y_val = val_input_df['melt_m_ice_per_y']\n",
    "    x_val = val_input_df.drop_vars(['melt_m_ice_per_y'])\n",
    "\n",
    "    #print('x_train : ',dfmt.print_shape_xr_ds(x_train), 'y_train : ',len(y_train))\n",
    "    #print('x_val  : ',dfmt.print_shape_xr_ds(x_val),  'y_test  : ',len(y_val))\n",
    "\n",
    "    print('here7')\n",
    "    ## normalise\n",
    "    norm_summary_list = []\n",
    "\n",
    "    for norm_method in ['std','interquart','minmax']:\n",
    "\n",
    "        summary_ds = compute_norm_metrics(x_train, y_train, norm_method)\n",
    "        norm_summary_list.append(summary_ds)\n",
    "\n",
    "    print('here8')\n",
    "    summary_ds_all = xr.concat(norm_summary_list, dim='norm_method')\n",
    "    \n",
    "    print('here9')\n",
    "    var_mean = summary_ds_all.sel(metric='mean_vars')\n",
    "    var_range = summary_ds_all.sel(metric='range_vars')\n",
    "\n",
    "    print('here10')\n",
    "    var_train_norm = (train_input_df - var_mean)/var_range\n",
    "    var_val_norm = (val_input_df - var_mean)/var_range\n",
    "    \n",
    "    return summary_ds_all, var_train_norm, var_val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141148ac-86c2-485f-8858-a760d5ea9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "tblock_dim = np.arange(1,17).tolist()+np.arange(21,50).tolist()\n",
    "isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]\n",
    "TS_opt = 'extrap'\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    outputpath_CVinput = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "elif TS_opt == 'whole':\n",
    "    outputpath_CVinput = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "elif TS_opt == 'thermocline':\n",
    "    outputpath_CVinput = inputpath_data+'THERMOCLINE_CHUNKS/'\n",
    "\n",
    "tblock_out = 0\n",
    "isf_out = 0\n",
    "metrics_ds, var_train_norm, var_val_norm = indat.prepare_input_data_CV(tblock_dim, isf_dim, tblock_out, isf_out, TS_opt, inputpath_data)\n",
    "#metrics_ds = indat.prepare_input_data_CV_onlymetrics(tblock_dim, isf_dim, tblock_out, isf_out, TS_opt, inputpath_data)\n",
    "metrics_ds.to_netcdf(outputpath_CVinput + 'metrics_norm_wholedataset_orig_christoph.nc')\n",
    "var_train_norm.to_netcdf(outputpath_CVinput + 'train_data_wholedataset_orig_christoph.nc')\n",
    "var_val_norm.to_netcdf(outputpath_CVinput + 'val_data_wholedataset_orig_christoph.nc')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ae28e-9a24-4426-802e-2dcd8a38e59f",
   "metadata": {},
   "source": [
    "PREPARE ADDITIONAL INPUT VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec856a-6af3-40d5-8bb6-4382326069b8",
   "metadata": {},
   "source": [
    "FOR WHOLE PROFILES, PUT ALL CHUNKS TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89e323-152a-465c-b917-b9d5ff1d5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "## which profile option are we using for temperature and salinity\n",
    "if TS_opt == 'extrap':\n",
    "    inputpath_prof = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS/'\n",
    "elif TS_opt == 'whole':\n",
    "    inputpath_prof = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "elif TS_opt == 'thermocline':\n",
    "    inputpath_prof = inputpath_data+'THERMOCLINE_CHUNKS/'\n",
    "\n",
    "train_input_df = None    \n",
    "tblock_list = []\n",
    "isf_list = []\n",
    "\n",
    "for tt in tblock_dim:\n",
    "    print(tt)\n",
    "\n",
    "    for kisf in isf_dim: \n",
    "\n",
    "        clean_df_nrun_kisf = pd.read_csv(inputpath_prof + 'dataframe_input_isf'+str(kisf).zfill(3)+'_'+str(tt).zfill(3)+'.csv',index_col=[0,1,2])\n",
    "        clean_df_nrun_kisf.reset_index(drop=True, inplace=True)\n",
    "        clean_ds_nrun_kisf = clean_df_nrun_kisf.to_xarray()\n",
    "\n",
    "        if train_input_df is None:\n",
    "            train_input_df = clean_ds_nrun_kisf.copy()\n",
    "            tblock_list = tblock_list + (np.zeros(len(train_input_df.index)) + tt).astype(int).tolist()\n",
    "            isf_list = isf_list + (np.zeros(len(train_input_df.index)) + kisf).astype(int).tolist()\n",
    "        else:\n",
    "            new_index = clean_ds_nrun_kisf.index.values + train_input_df.index.max().values+1\n",
    "            clean_ds_nrun_kisf = clean_ds_nrun_kisf.assign_coords({'index': new_index})\n",
    "            train_input_df = xr.concat([train_input_df, clean_ds_nrun_kisf], dim='index')\n",
    "            tblock_list = tblock_list + (np.zeros(len(new_index)) + tt).astype(int).tolist()\n",
    "            isf_list = isf_list + (np.zeros(len(new_index)) + kisf).astype(int).tolist()\n",
    "\n",
    "train_input_df.to_netcdf(inputpath_prof + 'dataframe_allisf_tblocks1to13.nc')\n",
    "index_ds = xr.Dataset({'Nisf': (['index'], isf_list), 'tblock': (['index'], tblock_list)}, coords={'index': train_input_df.index})\n",
    "index_ds.to_netcdf(inputpath_prof + 'indexing_allisf_tblocks1to13.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d27390-38a1-4c30-b46a-a784a49d5ae1",
   "metadata": {},
   "source": [
    "CV over shelves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85410be-433f-461c-91ec-c9abd8534dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tblock_dim = range(1,14)\n",
    "#isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]\n",
    "isf_dim = [38,55,58,61,65,66,69,70,71,73,75]\n",
    "TS_opt = 'whole'\n",
    "norm_method = 'std'\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    outputpath_CVinput = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS_CV/'\n",
    "elif TS_opt == 'whole':\n",
    "    outputpath_CVinput = inputpath_data+'WHOLE_PROF_CHUNKS_CV/'\n",
    "elif TS_opt == 'thermocline':\n",
    "    outputpath_CVinput = inputpath_data+'THERMOCLINE_CHUNKS_CV/'\n",
    "\n",
    "inputpath_prof = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "ds_all = xr.open_mfdataset(inputpath_prof + 'dataframe_allisf_tblocks1to13.nc').chunk({'index': 1e6})\n",
    "ds_idx = xr.open_mfdataset(inputpath_prof + 'indexing_allisf_tblocks1to13.nc').chunk({'index': 1e6})\n",
    "\n",
    "#seems to work, I could even increase the chunk size probably\n",
    "for isf_out in tqdm(isf_dim):\n",
    "    \n",
    "    print(isf_out)\n",
    "    tblock_out = 0\n",
    "    print('here')\n",
    "    data_train_norm, data_val_norm = indat.prepare_normed_input_data_CV_metricsgiven(tblock_dim, isf_dim, tblock_out, isf_out, TS_opt, inputpath_data, norm_method, ds_all=ds_all, ds_idx=ds_idx)\n",
    "    print('here1')\n",
    "    data_train_norm.to_netcdf(outputpath_CVinput + 'train_data_CV_norm'+norm_method+'_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc')\n",
    "    print('here2')\n",
    "    data_val_norm.to_netcdf(outputpath_CVinput + 'val_data_CV_norm'+norm_method+'_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc')\n",
    "    del data_train_norm\n",
    "    del data_val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26ac10-60ea-4bef-b70e-73dd28e74ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa49860-f3ab-4c94-aef6-c80bdfd4b372",
   "metadata": {},
   "source": [
    "CV over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67267e-b94e-4966-a49a-7afffd18f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "tblock_dim = range(6,14) #range(1,14)\n",
    "isf_dim = [10,11,12,13,18,22,23,24,25,30,31,33,38,39,40,42,43,44,45,47,48,51,52,53,54,55,58,61,65,66,69,70,71,73,75]\n",
    "TS_opt = 'whole'\n",
    "norm_method = 'std'\n",
    "# CONTINUE AT 6\n",
    "\n",
    "if TS_opt == 'extrap':\n",
    "    outputpath_CVinput = inputpath_data+'EXTRAPOLATED_ISFDRAFT_CHUNKS_CV/'\n",
    "elif TS_opt == 'whole':\n",
    "    outputpath_CVinput = inputpath_data+'WHOLE_PROF_CHUNKS_CV/'\n",
    "elif TS_opt == 'thermocline':\n",
    "    outputpath_CVinput = inputpath_data+'THERMOCLINE_CHUNKS_CV/'\n",
    "\n",
    "inputpath_prof = inputpath_data+'WHOLE_PROF_CHUNKS/'\n",
    "ds_all = xr.open_mfdataset(inputpath_prof + 'dataframe_allisf_tblocks1to13.nc').chunk({'index': 1e6})\n",
    "ds_idx = xr.open_mfdataset(inputpath_prof + 'indexing_allisf_tblocks1to13.nc').chunk({'index': 1e6})\n",
    "\n",
    "for tblock_out in tqdm(tblock_dim):\n",
    "\n",
    "    print(tblock_out)\n",
    "    isf_out = 0\n",
    "    print('here')\n",
    "    data_train_norm, data_val_norm = indat.prepare_normed_input_data_CV_metricsgiven(tblock_dim, isf_dim, tblock_out, isf_out, TS_opt, inputpath_data, norm_method, ds_all=ds_all, ds_idx=ds_idx)\n",
    "    print('here1')\n",
    "    data_train_norm.to_netcdf(outputpath_CVinput + 'train_data_CV_norm'+norm_method+'_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc')\n",
    "    print('here2')\n",
    "    data_val_norm.to_netcdf(outputpath_CVinput + 'val_data_CV_norm'+norm_method+'_noisf'+str(isf_out).zfill(3)+'_notblock'+str(tblock_out).zfill(3)+'.nc')\n",
    "    del data_train_norm\n",
    "    del data_val_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
