{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Fri Jan 12 11:21 2024\n",
    "\n",
    "Wanted to look if can avoid the ice shelf divisions for plumes\n",
    "@author: Clara Burgard\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multimelt.plume_functions as pf\n",
    "import multimelt.useful_functions as uf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_lim = [-3000000,3000000]\n",
    "\n",
    "#chunk_size = 700\n",
    "chunk_size = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data='/bettik/burgardc/DATA/SUMMER_PAPER/interim/'\n",
    "inputpath_metadata='/bettik/burgardc/SCRIPTS/basal_melt_param/data/raw/MASK_METADATA/'\n",
    "outputpath_mask ='/bettik/burgardc/DATA/SUMMER_PAPER/interim/ANTARCTICA_IS_MASKS/BedMachine_4km/'\n",
    "outputpath_boxes = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/BOXES/BedMachine_4km/'\n",
    "outputpath_plumes = '/bettik/burgardc/DATA/SUMMER_PAPER/interim/PLUMES/BedMachine_4km/'\n",
    "\n",
    "file_mask_orig = xr.open_dataset(inputpath_data+'BedMachine_v2_aggregated4km_allvars.nc')\n",
    "file_mask_orig_cut = uf.cut_domain_stereo(file_mask_orig, map_lim, map_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_msk = file_mask_orig_cut['mask_0_1_2']  #0 = ocean, 1 = ice shelves, 2 = grounded ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bed_orig = -1*file_mask_orig_cut['bed']\n",
    "file_draft = (file_mask_orig_cut['thickness'] - file_mask_orig_cut['surface']).where(file_msk==1)\n",
    "file_draft = file_draft.where(file_draft > 0, 0)\n",
    "file_isf_conc = file_mask_orig_cut['isf_conc']\n",
    "file_msk = file_mask_orig_cut['mask_0_1_2']  #0 = ocean, 1 = ice shelves, 2 = grounded ice\n",
    "\n",
    "\n",
    "xx = file_mask_orig_cut['x']\n",
    "yy = file_mask_orig_cut['y']\n",
    "\n",
    "xx = file_mask_orig_cut['x']\n",
    "yy = file_mask_orig_cut['y']\n",
    "\n",
    "dx = abs(xx[1] - xx[0])\n",
    "dy = abs(yy[1] - yy[0])\n",
    "\n",
    "file_isf = xr.open_dataset(outputpath_mask + 'BedMachinev2_4km_isf_masks_and_info_and_distance_oneFRIS.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_draft_pos = file_draft\n",
    "# Be careful with ice shelf 178 and 195 - they have a negative ice draft\n",
    "# I don't know how to fix it at the moment so I put it to nan\n",
    "#ice_draft_pos = ice_draft_pos.where(plume_var_of_int['ISF_mask'] != 178, np.nan)\n",
    "#ice_draft_pos = ice_draft_pos.where(plume_var_of_int['ISF_mask'] != 195, np.nan)\n",
    "\n",
    "ice_draft_neg = -1*ice_draft_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_plumes = '/bettik/burgardc/DATA/ERWIN_PAPER/interim/PLUMES/'\n",
    "\n",
    "plume_charac = xr.open_dataset(outputpath_plumes+'BedMachine_4km_plume_characteristics.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plume_charac['alpha'].sel(option='lazero') == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plume_charac['zGL'].sel(option='lazero').where((plume_charac['alpha'].sel(option='lazero') > 0) & (file_isf['ISF_mask'] > 1)).plot(vmax=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "((plume_charac['alpha'].sel(option='lazero') == 0) & (file_isf['ISF_mask'] > 1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazero_GL_alpha_kisf_newmethod2(kisf, ice_draft_neg_isf, GL_mask, isf_and_GL_mask, gl_mask_isl, dist_incl, weights8_0, weights16_0, mid_coord, sn_isf, first_crit, sn_isf_corr, first_crit_corr):\n",
    "    \"\"\"\n",
    "    \n",
    "    This function computes the plume departing grounding line depth and the local angle in a smoother manner than Lazeroms et al. 2018. \n",
    "    Remains heavily inspired from Lazeroms et al. 2018 (using the 16 directions).\n",
    "    Includes an option to extend the grounding line to neighboring points in case the original grounding line is weirdly shallow.\n",
    "    \n",
    "    kisf : int\n",
    "        ID of the ice shelf of interest\n",
    "    ice_draft_neg : xr.DataArray\n",
    "        Ice draft depth (Negative with depth!)\n",
    "    GL_mask : xr.DataArray\n",
    "        Mask of the Antarctic grounding lines\n",
    "    isf_and_GL_mask : xr.DataArray  \n",
    "        Mask of the isf and associated GL\n",
    "    dist_incl : int\n",
    "        Distance, in grid cells, to count within the grounding line\n",
    "    weights8_0 : xr.Dataset\n",
    "        Contains the weights (0,1) to look at the 8 neighbours of a point\n",
    "    weights16_0 : xr.Dataset\n",
    "        Contains the weights (0,1) to look in the 16 directions, starting at the point\n",
    "    mid_coord : int\n",
    "        Indication on how many times to propagate the grounding line\n",
    "    sn_isf : xr.DataArray\n",
    "        Slopes\n",
    "    first_crit : xr.DataArray\n",
    "       First criterion\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Enlarge GL mask to dist_incl rows (e.g. if your initial GL is shallow)\n",
    "    GL_mask1_0 = (GL_mask == kisf)\n",
    "    GL_2_mask = GL_mask1_0\n",
    "    for n in range(dist_incl):\n",
    "        GL_2 = pf.xr_nd_corr(GL_2_mask, weights8_0['weights'])\n",
    "        GL_2_sum = GL_2.sum('direction').where(isf_and_GL_mask == kisf)\n",
    "        GL_2_mask = (GL_2_sum > 0).astype(int)\n",
    "        \n",
    "    # Cut out the GL band in draft depth\n",
    "    GL_depth_isf = -1*(ice_draft_neg_isf.where(GL_2_mask))\n",
    "    \n",
    "\n",
    "\n",
    "    # Initialise the field at grounding line\n",
    "    GL_neighbors_new = GL_depth_isf\n",
    "    sn_new = sn_isf.where(first_crit).mean('direction')\n",
    "    sn_new = sn_new.where(sn_new > 0,0).where(GL_2_mask > 0)\n",
    "\n",
    "    second_crit_all = GL_depth_isf * 0 + 1\n",
    "\n",
    "    diff_masks = 1\n",
    "    i = 0\n",
    "    diff_stop = 0    \n",
    "\n",
    "\n",
    "    #for ii in range(20):\n",
    "    while diff_stop < 3:\n",
    "\n",
    "        mask_old_domain = np.isnan(GL_neighbors_new)\n",
    "\n",
    "        GL_neighbors = pf.xr_nd_corr(GL_neighbors_new, weights16_0['weights'])\n",
    "\n",
    "        # cut out the newly formed data strip\n",
    "        GL_neighbors_step = GL_neighbors.where(np.isnan(GL_neighbors_new))\n",
    "        GL_neighbors_step = GL_neighbors_step.where(isf_and_GL_mask == kisf)\n",
    "\n",
    "        # check if the propagated GL is deeper than point\n",
    "        diff_base_GL = (-1*ice_draft_neg_isf - GL_neighbors_step)\n",
    "        second_crit_n = diff_base_GL < 0 #<=\n",
    "\n",
    "        # combine this criterion and the slope criterion\n",
    "        all_crit =  first_crit & second_crit_n #\n",
    "\n",
    "        if diff_masks != 0:\n",
    "\n",
    "            # make a mean over all valid GL depths\n",
    "            GL_mean = GL_neighbors_step.where(all_crit).mean('direction')\n",
    "            GL_neighbors_new = GL_neighbors_new.where(GL_neighbors_new > 0,GL_mean)    \n",
    "\n",
    "            # make a mean over all valid slopes\n",
    "            sn_mean = sn_isf.where(all_crit).mean('direction')\n",
    "            sn_new = sn_new.where(sn_new > 0,sn_mean)\n",
    "\n",
    "            second_crit_all = second_crit_all.where(second_crit_all > 0,second_crit_n)   \n",
    "\n",
    "            diff_stop = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            #print('Entering obstacle option')\n",
    "\n",
    "            # insert corrected sn and first crit\n",
    "            first_crit_corr2 = first_crit.where((all_crit.sum('direction') > 0), first_crit_corr)\n",
    "            all_crit_corr = (first_crit_corr2 & second_crit_n).where(np.isfinite(GL_neighbors_step))\n",
    "            sn_isf_corr2 = sn_isf.where((all_crit.sum('direction') > 0), sn_isf_corr).where(np.isfinite(GL_neighbors_step))\n",
    "\n",
    "            # make a mean over all valid GL depths\n",
    "            GL_mean = GL_neighbors_step.where(all_crit_corr).mean('direction')\n",
    "            GL_neighbors_new = GL_neighbors_new.where(GL_neighbors_new > 0,GL_mean)    \n",
    "\n",
    "            # make a mean over all valid slopes\n",
    "            sn_mean = sn_isf_corr2.where(all_crit_corr).mean('direction')\n",
    "            sn_new = sn_new.where(sn_new > 0,sn_mean)\n",
    "\n",
    "            second_crit_all = second_crit_all.where(second_crit_all > 0,second_crit_n)   \n",
    "            print('no island')\n",
    "\n",
    "        if diff_stop == 2:\n",
    "            print('island')\n",
    "            \n",
    "            \n",
    "            #return gl_mask_isl, GL_mask, isf_and_GL_mask, GL_neighbors_new, second_crit_all, sn_new\n",
    "            \n",
    "            # cut out areas that are nan and potentially are near a grounding line of an island\n",
    "            # modified this one below start_new_GL = (gl_mask_isl) &  ~(GL_mask == kisf) & (isf_and_GL_mask == kisf) #~(GL_neighbors_new > 0) \n",
    "            start_new_GL = (gl_mask_isl)  & ~(GL_mask == kisf) #& np.isnan(sn_new)\n",
    "\n",
    "            GL_neighbors_new2 = -1*(ice_draft_neg_isf.where(start_new_GL))\n",
    "            sn_new2 = sn_isf.where(first_crit).mean('direction')\n",
    "            sn_new2 = sn_new2.where(sn_new2 > 0,0).where(start_new_GL > 0)\n",
    "            second_crit_all2 = GL_neighbors_new2 * 0 + 1\n",
    "            \n",
    "            mask_old_domain2 = np.isnan(GL_neighbors_new2)\n",
    "            \n",
    "            diff_masks2 = 1\n",
    "\n",
    "            for n in range(50):\n",
    "\n",
    "                GL_neighbors = pf.xr_nd_corr(GL_neighbors_new2, weights16_0['weights'])\n",
    "\n",
    "                # cut out the newly formed data strip\n",
    "                GL_neighbors_step = GL_neighbors.where(np.isnan(GL_neighbors_new2))\n",
    "                GL_neighbors_step = GL_neighbors_step.where(isf_and_GL_mask == kisf)\n",
    "\n",
    "                # check if the propagated GL is deeper than point\n",
    "                diff_base_GL = (-1*ice_draft_neg_isf - GL_neighbors_step)\n",
    "                second_crit_n = diff_base_GL < 0 #<=\n",
    "\n",
    "                # combine this criterion and the slope criterion\n",
    "                all_crit =  first_crit & second_crit_n #\n",
    "                \n",
    "                if diff_masks2 != 0 :\n",
    "                    # make a mean over all valid GL depths\n",
    "                    GL_mean = GL_neighbors_step.where(all_crit).mean('direction')\n",
    "                    GL_neighbors_new2 = GL_neighbors_new2.where(GL_neighbors_new2 > 0,GL_mean)    \n",
    "\n",
    "                    # make a mean over all valid slopes\n",
    "                    sn_mean = sn_isf.where(all_crit).mean('direction')\n",
    "                    sn_new2 = sn_new2.where(sn_new2 > 0,sn_mean)\n",
    "\n",
    "                    second_crit_all2 = second_crit_all2.where(second_crit_all2 > 0,second_crit_n)   \n",
    "                    mask_new_domain2 = np.isnan(GL_neighbors_new2)\n",
    "                    diff_masks2 = (mask_new_domain2.astype(int) - mask_old_domain2.astype(int)).sum().values\n",
    "                    \n",
    "                else:\n",
    "                \n",
    "                    # insert corrected sn and first crit\n",
    "                    first_crit_corr2 = first_crit.where((all_crit.sum('direction') > 0), first_crit_corr)\n",
    "                    all_crit_corr = (first_crit_corr2 & second_crit_n).where(np.isfinite(GL_neighbors_step))\n",
    "                    sn_isf_corr2 = sn_isf.where((all_crit.sum('direction') > 0), sn_isf_corr).where(np.isfinite(GL_neighbors_step))\n",
    "\n",
    "                    # make a mean over all valid GL depths\n",
    "                    GL_mean = GL_neighbors_step.where(all_crit_corr).mean('direction')\n",
    "                    GL_neighbors_new2 = GL_neighbors_new2.where(GL_neighbors_new2 > 0,GL_mean)    \n",
    "\n",
    "                    # make a mean over all valid slopes\n",
    "                    sn_mean = sn_isf_corr2.where(all_crit_corr).mean('direction')\n",
    "                    sn_new2 = sn_new2.where(sn_new2 > 0,sn_mean)\n",
    "\n",
    "                    second_crit_all2 = second_crit_all2.where(second_crit_all2 > 0,second_crit_n)\n",
    "                    \n",
    "                # fill nans with this new product\n",
    "                GL_neighbors_new = GL_neighbors_new.combine_first(GL_neighbors_new2)\n",
    "                sn_new = sn_new.combine_first(sn_new2)\n",
    "            #return sn_new2, sn_mean\n",
    "\n",
    "                ## fill nans with this new product\n",
    "                #GL_neighbors_new = GL_neighbors_new.where(np.isfinite(GL_neighbors_new), GL_neighbors_new2)\n",
    "            \n",
    "        # check if we still have obstacles\n",
    "        mask_new_domain = np.isnan(GL_neighbors_new)\n",
    "        diff_masks = (mask_new_domain.astype(int) - mask_old_domain.astype(int)).sum().values\n",
    "\n",
    "        # check if we have reached the maximum\n",
    "        diff_mask_isf = np.isnan(GL_neighbors_new) & (isf_and_GL_mask == kisf)\n",
    "        \n",
    "        if diff_masks == 0:\n",
    "            #print('mask did not change', diff_stop)\n",
    "            diff_stop = diff_stop+1\n",
    "                    \n",
    "        i = i+1\n",
    "        \n",
    "        if i == 500:\n",
    "            return  np.arctan(sn_new), -1*GL_neighbors_new\n",
    "            print('reached 500 iterations')\n",
    "            break\n",
    "        \n",
    "            \n",
    "    return  np.arctan(sn_new), -1*GL_neighbors_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_criterion_lazero_general(kisf, plume_var_of_int, ice_draft_neg_isf, isf_and_GL_mask, ds_weights, dx, dy, dir_nb=16, grad_corr=0, extra_shift=2):\n",
    "\n",
    "    \"\"\"\n",
    "    Define first criterion for the plume parameters using a smoother version of the 16 directions and permitting to use a different amount of directions\n",
    "    \n",
    "    This function computes the basal slope and identifies the first criterion, following the methodology in Lazeroms et al;, 2018.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kisf : int\n",
    "        ID of the ice shelf of interest\n",
    "    plume_var_of_int : xr.Dataset\n",
    "        Dataset containing ``'ISF_mask'`` and ``'GL_mask'``\n",
    "    ice_draft_neg_isf : xr.DataArray\n",
    "        Ice draft depth for the given ice shelf in m. Negative downwards.\n",
    "    isf_and_GL_mask : xr.DataArray\n",
    "        Mask of the domain covered by the ice shelf and the grounding line (this extra mask is needed if the grounding line is defined on ground points)\n",
    "    ds_weights : xr.Dataset\n",
    "        Weights for the filter and information about the x- and y-shift in the 16 directions.\n",
    "    dx : float\n",
    "        Grid spacing in the x-direction\n",
    "    dy : float\n",
    "        Grid spacing in the y-direction\n",
    "    dir_nb: int\n",
    "        Amount of directions used. I tried with 8, 16, 24. Decided to stay with 16.\n",
    "    grad_corr: int\n",
    "        If we want to add some uncertainty in the slopes (adds grad_corr to the gradient) => makes it easier to have positive slopes when the differences are tiny.\n",
    "    extra_shift: int\n",
    "        Should be 2 if you do the smooth version, otherwise 1.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    GL_depth : xr.DataArray\n",
    "        Depth of the grounding line points (negative downwards).\n",
    "    sn_isf : xr.DataArray\n",
    "        Basal slope in all 16 directions\n",
    "    first_crit : xr.DataArray\n",
    "        Boolean where sn_sf > 0\n",
    "    draft_depth : \n",
    "        Ice draft depth in m (negative downwards) extended through the 'direction' dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    # add dimension for directions to the ice_draft array\n",
    "    other = xr.DataArray(np.zeros(dir_nb), coords=[('direction', np.arange(dir_nb))])\n",
    "    ice_draft_neg_dirs, other2 = xr.broadcast(ice_draft_neg_isf, other)\n",
    "\n",
    "    # draft depth only on the ice shelf\n",
    "    draft_depth = ice_draft_neg_dirs.where(isf_and_GL_mask).where((plume_var_of_int['ISF_mask'] == kisf))\n",
    "\n",
    "    # grounding line depth only where grounding line\n",
    "    GL_depth = ice_draft_neg_dirs.where(isf_and_GL_mask).where(plume_var_of_int['GL_mask'] == kisf)\n",
    "    GL_depth = GL_depth.where(GL_depth < 0, 0)\n",
    "    \n",
    "    # apply the correlation filter to compute gradients in the 16 directions (xr_nd_corr_sig does not work for whatever reason :( ))\n",
    "    gradients = pf.xr_nd_corr(draft_depth, ds_weights['weights'])\n",
    "\n",
    "    # compute the sn - basal slope - to be consistent with the origin of the plumes, we cut the basal slopes after ice shelves as well - but might need to think about what happens when several ice shelves are touching each other\n",
    "    sn_isf = gradients / np.sqrt((ds_weights['shift_x'] * extra_shift * np.abs(dx)) ** 2 + (ds_weights['shift_y'] * extra_shift * np.abs(dy)) ** 2)\n",
    "    # adding correction for criterion\n",
    "    sn_isf_corr = (gradients +  grad_corr) / np.sqrt((ds_weights['shift_x'] * extra_shift * np.abs(dx)) ** 2 + (ds_weights['shift_y'] * extra_shift * np.abs(dy)) ** 2)\n",
    "    # 1st criterion: sn > 0\n",
    "    first_crit = sn_isf > 0\n",
    "    first_crit_corr = sn_isf_corr > 0\n",
    "\n",
    "    return sn_isf, sn_isf_corr, first_crit, first_crit_corr\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "kisf = 42\n",
    "extra_shift = 1\n",
    "plume_var_of_int = file_isf\n",
    "dir_nb = 16\n",
    "grad_corr = 6\n",
    "dist_incl= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights8 = pf.create_8_dir_weights()\n",
    "weights16 = pf.create_16_dir_weights()\n",
    "\n",
    "if dir_nb == 16:\n",
    "    if extra_shift == 2:\n",
    "        weights_across = pf.create_16_dir_weights_across()\n",
    "    elif extra_shift == 1:\n",
    "        weights_across = pf.create_16_dir_weights()\n",
    "elif dir_nb == 8:\n",
    "    if extra_shift == 2:\n",
    "        weights_across = pf.create_8_dir_weights_across()\n",
    "    elif extra_shift == 1:\n",
    "        weights_across = pf.create_8_dir_weights()    \n",
    "\n",
    "\n",
    "weights8_0 = weights8.where(weights8 < 0,0) * -1\n",
    "weights8_0 = weights8_0.where(weights8_0 > 0,0)\n",
    "\n",
    "weights16_0 = weights16.where(weights16 < 0,0) * -1\n",
    "weights16_0 = weights16_0.where(weights16_0 > 0,0)\n",
    "\n",
    "\n",
    "# prepare mask for whole domain (GL + ice shelf)\n",
    "plume_var_of_int['GL_and_ISF_mask'] = plume_var_of_int['GL_mask'].combine_first(plume_var_of_int['ISF_mask'])\n",
    "isf_and_GL_mask = plume_var_of_int['GL_and_ISF_mask'].where(\n",
    "    (plume_var_of_int['ISF_mask'] == kisf) | (plume_var_of_int['GL_mask'] == kisf)).dropna(how='all',dim='x').dropna(how='all', dim='y')\n",
    "ice_draft_neg_isf = ice_draft_neg.where(isf_and_GL_mask == kisf)\n",
    "\n",
    "# first crit    \n",
    "#draft_depth, ds_weights = first_criterion_lazero_general(kisf, plume_var_of_int, ice_draft_neg_isf, isf_and_GL_mask, weights_across, dx, dy, dir_nb=dir_nb, grad_corr=grad_corr, extra_shift=extra_shift) \n",
    "sn_isf, sn_isf_corr, first_crit, first_crit_corr = pf.first_criterion_lazero_general(kisf, plume_var_of_int, ice_draft_neg_isf, isf_and_GL_mask, weights_across, dx, dy, dir_nb=dir_nb, grad_corr=grad_corr, extra_shift=extra_shift) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "corr_mask_max.where(file_isf['ISF_mask'] == 11, drop=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights4 = pf.create_4_dir_weights()\n",
    "mask_0_1_2 = plume_var_of_int['ISF_mask'].where(plume_var_of_int['ISF_mask'] < 2,2)\n",
    "corr_mask = pf.xr_nd_corr(mask_0_1_2, weights4['weights'])\n",
    "corr_mask_max = np.abs(corr_mask).max('direction')\n",
    "plume_var_of_int['GL_mask_with_isl'] = (corr_mask_max == 2)\n",
    "\n",
    "\n",
    "#gl_mask_isl, GL_mask, isf_and_GL_mask, GL_neighbors_new, second_crit_all, sn_new \n",
    "alpha, zGL = pf.lazero_GL_alpha_kisf_newmethod2(kisf, \n",
    "                                             ice_draft_neg_isf, \n",
    "                                             plume_var_of_int['GL_mask'], \n",
    "                                             isf_and_GL_mask, \n",
    "                                             plume_var_of_int['GL_mask_with_isl'], \n",
    "                                             dist_incl, \n",
    "                                             weights8_0, \n",
    "                                             weights16_0, \n",
    "                                             200, \n",
    "                                             sn_isf, \n",
    "                                             first_crit, \n",
    "                                             sn_isf_corr, \n",
    "                                             first_crit_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights4 = pf.create_4_dir_weights()\n",
    "mask_0_1_2 = plume_var_of_int['ISF_mask'].where(plume_var_of_int['ISF_mask'] < 2,2)\n",
    "corr_mask = pf.xr_nd_corr(mask_0_1_2, weights4['weights'])\n",
    "corr_mask_max = np.abs(corr_mask).max('direction')\n",
    "plume_var_of_int['GL_mask_with_isl'] = (corr_mask_max == 2)\n",
    "\n",
    "alpha, zGL = pf.compute_zGL_alpha_lazero_newmethod(kisf, plume_var_of_int, ice_draft_neg, dx, dy, dir_nb, grad_corr, extra_shift, dist_incl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = 'new_lazero'\n",
    "alpha, zGL = pf.compute_zGL_alpha_all(plume_var_of_int, opt, ice_draft_neg, grad_corr=grad_corr, dir_nb=dir_nb, extra_shift=extra_shift, dist_incl=dist_incl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plum_charac = pf.prepare_plume_charac([opt], ice_draft_pos, plume_var_of_int, grad_corr=grad_corr, dir_nb=dir_nb, extra_shift=extra_shift, dist_incl=dist_incl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha_kisf = alpha.where(np.isfinite(alpha), 0)\n",
    "#zGL_kisf = zGL.where(np.isfinite(zGL), ice_draft_neg_isf)\n",
    "\n",
    "go_back_to_whole_grid_alpha = alpha.reindex_like(plume_var_of_int['ISF_mask'])\n",
    "go_back_to_whole_grid_zgl = zGL.reindex_like(plume_var_of_int['ISF_mask'])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plum_charac['alpha'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_isf['ISF_mask'].plot(vmin=41,vmax=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.plot(vmax=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "((alpha == 0) & (file_isf['ISF_mask'] == kisf)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.where(np.isfinite(alpha), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "((alpha_kisf == 0) & (file_isf['ISF_mask'] == kisf)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "file_isf['ISF_mask'].where(file_isf['ISF_mask'] == kisf, drop=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dd in range(4):\n",
    "    plt.figure()\n",
    "    np.abs(corr_mask).isel(direction=dd).where(file_isf['ISF_mask'] == 11, drop=True).plot()\n",
    "    plt.title(str(dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "np.abs(corr_mask).max('direction').where(file_isf['ISF_mask'] == 11, drop=True).plot()\n",
    "plt.title(str(dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "((file_isf['ISF_mask'] == kisf) & (alpha > 0)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "((gl_mask_isl) & ~(GL_mask == kisf)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut out areas that are nan and potentially are near a grounding line of an island\n",
    "start_new_GL = (gl_mask_isl) & ~(GL_mask == kisf)\n",
    "\n",
    "GL_neighbors_new2 = -1*(ice_draft_neg_isf.where(start_new_GL))\n",
    "sn_new2 = sn_isf.where(first_crit).mean('direction')\n",
    "sn_new2 = sn_new2.where(sn_new2 > 0,0).where(start_new_GL > 0)\n",
    "second_crit_all2 = GL_neighbors_new2 * 0 + 1\n",
    "\n",
    "mask_old_domain2 = np.isnan(GL_neighbors_new2)\n",
    "\n",
    "diff_masks2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "GL_neighbors = pf.xr_nd_corr(GL_neighbors_new2, weights16_0['weights'])\n",
    "\n",
    "# cut out the newly formed data strip\n",
    "GL_neighbors_step = GL_neighbors.where(np.isnan(GL_neighbors_new2))\n",
    "GL_neighbors_step = GL_neighbors_step.where(isf_and_GL_mask == kisf)\n",
    "\n",
    "# check if the propagated GL is deeper than point\n",
    "diff_base_GL = (-1*ice_draft_neg_isf - GL_neighbors_step)\n",
    "second_crit_n = diff_base_GL < 0 #<=\n",
    "\n",
    "# combine this criterion and the slope criterion\n",
    "all_crit =  first_crit & second_crit_n #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(50):\n",
    "\n",
    "    GL_neighbors = pf.xr_nd_corr(GL_neighbors_new2, weights16_0['weights'])\n",
    "\n",
    "    # cut out the newly formed data strip\n",
    "    GL_neighbors_step = GL_neighbors.where(np.isnan(GL_neighbors_new2))\n",
    "    GL_neighbors_step = GL_neighbors_step.where(isf_and_GL_mask == kisf)\n",
    "\n",
    "    # check if the propagated GL is deeper than point\n",
    "    diff_base_GL = (-1*ice_draft_neg_isf - GL_neighbors_step)\n",
    "    second_crit_n = diff_base_GL < 0 #<=\n",
    "\n",
    "    # combine this criterion and the slope criterion\n",
    "    all_crit =  first_crit & second_crit_n #\n",
    "\n",
    "    if diff_masks2 != 0 :\n",
    "        # make a mean over all valid GL depths\n",
    "        GL_mean = GL_neighbors_step.where(all_crit).mean('direction')\n",
    "        GL_neighbors_new2 = GL_neighbors_new2.where(GL_neighbors_new2 > 0,GL_mean)    \n",
    "\n",
    "        # make a mean over all valid slopes\n",
    "        sn_mean = sn_isf.where(all_crit).mean('direction')\n",
    "        sn_new2 = sn_new2.where(sn_new2 > 0,sn_mean)\n",
    "\n",
    "        second_crit_all2 = second_crit_all2.where(second_crit_all2 > 0,second_crit_n)   \n",
    "        mask_new_domain2 = np.isnan(GL_neighbors_new2)\n",
    "        diff_masks2 = (mask_new_domain2.astype(int) - mask_old_domain2.astype(int)).sum().values\n",
    "\n",
    "    else:\n",
    "\n",
    "        # insert corrected sn and first crit\n",
    "        first_crit_corr2 = first_crit.where((all_crit.sum('direction') > 0), first_crit_corr)\n",
    "        all_crit_corr = (first_crit_corr2 & second_crit_n).where(np.isfinite(GL_neighbors_step))\n",
    "        sn_isf_corr2 = sn_isf.where((all_crit.sum('direction') > 0), sn_isf_corr).where(np.isfinite(GL_neighbors_step))\n",
    "\n",
    "        # make a mean over all valid GL depths\n",
    "        GL_mean = GL_neighbors_step.where(all_crit_corr).mean('direction')\n",
    "        GL_neighbors_new2 = GL_neighbors_new2.where(GL_neighbors_new2 > 0,GL_mean)    \n",
    "\n",
    "        # make a mean over all valid slopes\n",
    "        sn_mean = sn_isf_corr2.where(all_crit_corr).mean('direction')\n",
    "        sn_new2 = sn_new2.where(sn_new2 > 0,sn_mean)\n",
    "\n",
    "        second_crit_all2 = second_crit_all2.where(second_crit_all2 > 0,second_crit_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_new2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "GL_neighbors_new_test = zGL.where(np.isfinite(zGL), alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "zGL.combine_first(alpha).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "GL_neighbors_new_test.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "alpha.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "zGL.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "zGL.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
